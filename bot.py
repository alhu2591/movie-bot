import subprocess
import sys # ØªÙ… Ø¥Ø¶Ø§ÙØ© Ù‡Ø°Ø§ Ø§Ù„Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ù„Ø­Ù„ Ù…Ø´ÙƒÙ„Ø© NameError
import os
import re
import sqlite3
import threading
import schedule
import time
import asyncio
from bs4 import BeautifulSoup
from telegram import Update, InlineKeyboardButton, InlineKeyboardMarkup, ReplyKeyboardMarkup, KeyboardButton
from telegram.ext import Application, CommandHandler, ContextTypes, CallbackQueryHandler, MessageHandler, filters
from flask import Flask
from urllib.parse import urlparse, urlunparse
from playwright.async_api import async_playwright
import site # ØªÙ… Ø¥Ø¶Ø§ÙØ© Ù‡Ø°Ø§ Ø§Ù„Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ù„ØªØ­Ø³ÙŠÙ† Ø§ÙƒØªØ´Ø§Ù Ù…Ø³Ø§Ø± Ø§Ù„Ù…ÙƒØªØ¨Ø§Øª

# --- Logging Setup ---
import logging
logging.basicConfig(
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    level=logging.INFO
)
logger = logging.getLogger(__name__)

# --- Add virtual environment site-packages to sys.path explicitly ---
# This is a more robust way to ensure all installed packages are discoverable at runtime,
# especially in environments like Render.com where default sys.path might be incomplete.
try:
    # sys.prefix points to the root of the virtual environment
    # Construct the path to site-packages based on common venv structure and Python version
    python_version_dir = f"python{sys.version_info.major}.{sys.version_info.minor}"
    site_packages_path = os.path.join(sys.prefix, 'lib', python_version_dir, 'site-packages')

    if os.path.isdir(site_packages_path) and site_packages_path not in sys.path:
        sys.path.insert(0, site_packages_path)
        logger.info(f"Added explicit site-packages path to sys.path: {site_packages_path}")
    else:
        logger.warning(f"Could not find or add site-packages path: {site_packages_path}")
except Exception as e:
    logger.critical(f"FATAL ERROR: Failed to configure sys.path for package discovery: {e}")
    sys.exit(1) # Exit early if we can't ensure packages are found


# --- Package Installation and Verification ---
def ensure_packages_installed():
    """
    ØªØªØ­Ù‚Ù‚ Ù…Ù† Ø£Ù† Ø§Ù„Ù…ÙƒØªØ¨Ø§Øª Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ© Ù‚Ø§Ø¨Ù„Ø© Ù„Ù„Ø§Ø³ØªÙŠØ±Ø§Ø¯.
    ØªÙØªØ±Ø¶ Ø£Ù† Ø§Ù„Ù…ÙƒØªØ¨Ø§Øª Ù…Ø«Ø¨ØªØ© Ø¹Ø¨Ø± requirements.txt Ø¨ÙˆØ§Ø³Ø·Ø© Ø¨ÙŠØ¦Ø© Ø§Ù„Ù†Ø´Ø±.
    """
    logger.info("Verifying critical imports...")
    try:
        # Ù…Ø­Ø§ÙˆÙ„Ø© Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„Ù…ÙƒØªØ¨Ø§Øª Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©
        import bs4 # ØªÙ… ØªØºÙŠÙŠØ± Ù‡Ø°Ø§ Ø§Ù„Ø³Ø·Ø± Ù…Ù† 'beautifulsoup4' Ø¥Ù„Ù‰ 'bs4'
        import lxml
        import python_telegram_bot
        import aiohttp
        import schedule
        import playwright # ØªÙ… Ø¥Ø¶Ø§ÙØ© playwright Ù„Ù„ØªØ­Ù‚Ù‚
        logger.info("âœ… Ø¬Ù…ÙŠØ¹ Ù…ÙƒØªØ¨Ø§Øª Python Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ© Ù‚Ø§Ø¨Ù„Ø© Ù„Ù„Ø§Ø³ØªÙŠØ±Ø§Ø¯.")
    except ImportError as e:
        logger.critical(f"âŒ Ø®Ø·Ø£ Ø­Ø±Ø¬ ÙÙŠ Ø§Ù„Ø§Ø³ØªÙŠØ±Ø§Ø¯: {e}")
        logger.critical("ÙˆØ§Ø­Ø¯ Ø£Ùˆ Ø£ÙƒØ«Ø± Ù…Ù† Ø§Ù„Ù…ÙƒØªØ¨Ø§Øª Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø© ØºÙŠØ± Ù…Ø«Ø¨Øª. ÙŠØ±Ø¬Ù‰ Ø§Ù„ØªØ£ÙƒØ¯ Ù…Ù† Ø£Ù† 'requirements.txt' ØµØ­ÙŠØ­ ÙˆØ£Ù† Ø§Ù„Ø§Ø¹ØªÙ…Ø§Ø¯Ø§Øª Ù…Ø«Ø¨ØªØ©.")
        sys.exit(1) # Ø§Ù„Ø®Ø±ÙˆØ¬ Ø¥Ø°Ø§ ÙØ´Ù„ Ø§Ù„Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„Ø­Ø±Ø¬

    try:
        # ØªÙ… Ù†Ù‚Ù„ Ù‡Ø°Ø§ Ø§Ù„Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø¥Ù„Ù‰ Ù‡Ù†Ø§ Ù„Ø¶Ù…Ø§Ù† Ø£Ù† 'telegram' Ù…ÙˆØ¬ÙˆØ¯ Ù‚Ø¨Ù„ Ù…Ø­Ø§ÙˆÙ„Ø© Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ù…ÙƒÙˆÙ†Ø§ØªÙ‡
        # Ù‡Ø°Ø§ ÙŠØ­Ù„ Ù…Ø´ÙƒÙ„Ø© Ù…Ø­ØªÙ…Ù„Ø© Ø¥Ø°Ø§ ÙƒØ§Ù† python-telegram-bot ØºÙŠØ± Ù…Ø«Ø¨Øª Ø¨Ø´ÙƒÙ„ ØµØ­ÙŠØ­
        from telegram import Update, InlineKeyboardButton, InlineKeyboardMarkup, ReplyKeyboardMarkup, KeyboardButton
        from telegram.ext import Application, CommandHandler, ContextTypes, CallbackQueryHandler, MessageHandler, filters
        logger.info("âœ… Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ù…ÙƒÙˆÙ†Ø§Øª Python-Telegram-Bot Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ© Ø¨Ù†Ø¬Ø§Ø­.")
    except ImportError as e:
        logger.critical(f"âŒ Ø®Ø·Ø£ Ø­Ø±Ø¬ ÙÙŠ Ø§Ù„Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ù„Ù…ÙƒÙˆÙ†Ø§Øª python-telegram-bot: {e}")
        logger.critical("Ù‡Ø°Ø§ ÙŠØ¹Ù†ÙŠ Ø¹Ø§Ø¯Ø©Ù‹ Ø£Ù† 'python-telegram-bot' ØºÙŠØ± Ù…Ø«Ø¨Øª Ø¨Ø´ÙƒÙ„ ØµØ­ÙŠØ­ Ø£Ùˆ Ø£Ù† Ù‡Ù†Ø§Ùƒ Ø­Ø²Ù…Ø© 'telegram' Ù…ØªØ¹Ø§Ø±Ø¶Ø© Ù…ÙˆØ¬ÙˆØ¯Ø©.")
        logger.critical("ØªØ£ÙƒØ¯ Ù…Ù† Ø£Ù†Ùƒ ØªØ³ØªØ®Ø¯Ù… Ø¥ØµØ¯Ø§Ø± Python Ù…ØªÙˆØ§ÙÙ‚ (Ù…Ø«Ù„ 3.11 Ø£Ùˆ 3.12) ÙˆØ£Ù† 'python-telegram-bot==20.7' Ù…ÙˆØ¬ÙˆØ¯ ÙÙŠ requirements.txt.")
        sys.exit(1)

# Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ Ø§Ù„Ø¯Ø§Ù„Ø© Ø¹Ù†Ø¯ Ø¨Ø¯Ø¡ ØªØ´ØºÙŠÙ„ Ø§Ù„Ø¨ÙˆØª
ensure_packages_installed()

# ØªØ£ÙƒØ¯ Ù…Ù† ØªØ«Ø¨ÙŠØª Ø§Ù„Ù…ØªØµÙØ­Ø§Øª ØªÙ„Ù‚Ø§Ø¦ÙŠØ§Ù‹ Ø¹Ù†Ø¯ Ø§Ù„ØªØ´ØºÙŠÙ„
def install_playwright_browsers():
    try:
        logger.info("Attempting to install Playwright browsers...")
        # Ø§Ø³ØªØ®Ø¯Ø§Ù… --with-deps Ù„Ø¶Ù…Ø§Ù† ØªØ«Ø¨ÙŠØª Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø§Ø¹ØªÙ…Ø§Ø¯Ø§Øª Ø§Ù„Ø¶Ø±ÙˆØ±ÙŠØ©
        # Ø§Ø³ØªØ®Ø¯Ø§Ù… sys.executable Ù„Ø¶Ù…Ø§Ù† Ø§Ø³ØªØ®Ø¯Ø§Ù… Python Ø§Ù„ØµØ­ÙŠØ­ ÙÙŠ Ø§Ù„Ø¨ÙŠØ¦Ø© Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ©
        result = subprocess.run([sys.executable, "-m", "playwright", "install", "--with-deps"], capture_output=True, text=True, check=True)
        logger.info("âœ… ØªÙ… ØªØ«Ø¨ÙŠØª Ù…ØªØµÙØ­Ø§Øª Playwright Ø¨Ù†Ø¬Ø§Ø­.")
        logger.debug(result.stdout)
    except subprocess.CalledProcessError as e:
        logger.critical(f"âŒ Ø®Ø·Ø£ ÙÙŠ ØªØ«Ø¨ÙŠØª Ù…ØªØµÙØ­Ø§Øª Playwright:\n{e.stderr}")
        sys.exit(1) # Ø§Ù„Ø®Ø±ÙˆØ¬ Ø¥Ø°Ø§ ÙØ´Ù„ ØªØ«Ø¨ÙŠØª Ø§Ù„Ù…ØªØµÙØ­
    except Exception as e:
        logger.critical(f"âŒ Ø­Ø¯Ø« Ø®Ø·Ø£ ØºÙŠØ± Ù…ØªÙˆÙ‚Ø¹ Ø£Ø«Ù†Ø§Ø¡ ØªØ«Ø¨ÙŠØª Ø§Ù„Ù…ØªØµÙØ­Ø§Øª: {e}")
        sys.exit(1)

# Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ Ø§Ù„Ø¯Ø§Ù„Ø© Ø¹Ù†Ø¯ Ø¨Ø¯Ø¡ ØªØ´ØºÙŠÙ„ Ø§Ù„Ø¨ÙˆØª
install_playwright_browsers()

# --- Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ø¨ÙˆØª ---
TOKEN = os.getenv("BOT_TOKEN", "7576844775:AAGyos4JkSNiiiwQ5oeCJdAw-2ajMkVdUUA") # ØªÙ… ØªØ­Ø¯ÙŠØ« Ù‡Ø°Ø§ Ø§Ù„Ø±Ù…Ø² Ø¨Ø±Ù…Ø² Ø§Ù„Ø¨ÙˆØª Ø§Ù„Ø¬Ø¯ÙŠØ¯ Ø§Ù„Ø®Ø§Øµ Ø¨Ùƒ.

# --- Ø¥Ø¹Ø¯Ø§Ø¯ Ø®Ø§Ø¯Ù… keep_alive ---
app = Flask(__name__)
@app.route('/')
def home():
    return "ğŸ¬ Ø¨ÙˆØª Ø§Ù„Ø£ÙÙ„Ø§Ù… ÙŠØ¹Ù…Ù„ Ø¨Ù†Ø¬Ø§Ø­! | 12 Ù…ÙˆÙ‚Ø¹ Ø³ÙŠÙ†Ù…Ø§Ø¦ÙŠ | ØªØ­Ø¯ÙŠØ« ÙƒÙ„ Ø³Ø§Ø¹Ø©"
def run_flask_app():
    app.run(host='0.0.0.0', port=8080)
threading.Thread(target=run_flask_app, daemon=True).start()

# --- ØªÙ‡ÙŠØ¦Ø© Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ---
def init_db():
    conn = sqlite3.connect('movies.db')
    c = conn.cursor()
    c.execute('''CREATE TABLE IF NOT EXISTS movies
                (id INTEGER PRIMARY KEY AUTOINCREMENT,
                 title TEXT NOT NULL,
                 url TEXT NOT NULL UNIQUE,
                 source TEXT NOT NULL,
                 image_url TEXT,
                 last_updated TIMESTAMP DEFAULT CURRENT_TIMESTAMP)''')
    # Add image_url column if it doesn't exist
    try:
        c.execute("ALTER TABLE movies ADD COLUMN image_url TEXT")
    except sqlite3.OperationalError as e:
        if "duplicate column name" not in str(e):
            logger.error(f"Error altering table: {e}")
    
    c.execute('''CREATE TABLE IF NOT EXISTS users
                (user_id INTEGER PRIMARY KEY,
                 username TEXT,
                 first_name TEXT,
                 last_name TEXT,
                 join_date TIMESTAMP DEFAULT CURRENT_TIMESTAMP)''')
    conn.commit()
    conn.close()

# --- Ø¥Ø¶Ø§ÙØ© Ù…Ø³ØªØ®Ø¯Ù… Ø¬Ø¯ÙŠØ¯ ---
def add_user(user_id, username, first_name, last_name):
    conn = sqlite3.connect('movies.db')
    c = conn.cursor()
    c.execute("INSERT OR IGNORE INTO users (user_id, username, first_name, last_name) VALUES (?, ?, ?, ?)",
              (user_id, username, first_name, last_name))
    conn.commit()
    conn.close()

# --- ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ø¹Ù†Ø§ÙˆÙŠÙ† ---
def clean_title(title):
    # Ø¥Ø²Ø§Ù„Ø© (Ø³Ù†Ø©) Ø£Ùˆ [Ø³Ù†Ø©] Ø£Ùˆ ÙƒÙ„Ù…Ø§Øª Ù…Ø«Ù„ "Ù…ØªØ±Ø¬Ù…" Ø£Ùˆ "Ø§ÙˆÙ† Ù„Ø§ÙŠÙ†"
    title = re.sub(r'\s*\(\d{4}\)|\s*\[.*?\]|\s*Ù…ØªØ±Ø¬Ù…|\s*Ø§ÙˆÙ† Ù„Ø§ÙŠÙ†|\s*online|\s*HD|\s*WEB-DL|\s*BluRay|\s*Ù†Ø³Ø®Ø© Ù…Ø¯Ø¨Ù„Ø¬Ø©', '', title, flags=re.IGNORECASE)
    # Ø¥Ø²Ø§Ù„Ø© Ø£ÙŠ Ø£Ø­Ø±Ù ØºÙŠØ± Ø£Ø¨Ø¬Ø¯ÙŠØ© Ø±Ù‚Ù…ÙŠØ© Ø£Ùˆ Ù…Ø³Ø§ÙØ§ØªØŒ Ø¨Ø§Ø³ØªØ«Ù†Ø§Ø¡ Ø§Ù„Ù…Ø³Ø§ÙØ§Øª
    title = re.sub(r'[^\w\s\u0600-\u06FF]+', '', title) # ÙŠØ¯Ø¹Ù… Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©
    # Ø§Ø³ØªØ¨Ø¯Ø§Ù„ Ù…Ø³Ø§ÙØ§Øª Ù…ØªØ¹Ø¯Ø¯Ø© Ø¨Ù…Ø³Ø§ÙØ© ÙˆØ§Ø­Ø¯Ø©
    title = re.sub(r'\s{2,}', ' ', title)
    return title.strip()


# --- Ø¯ÙˆØ§Ù„ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…ÙˆØ§Ù‚Ø¹ (ØªÙ… Ø§Ù„ØªØ­Ø¯ÙŠØ«) ---

def parse_wecima(soup):
    movies = []
    for item in soup.select("div.GridItem"):
        try:
            link_tag = item.select_one("a")
            if not link_tag or not link_tag.get("href"):
                logger.debug(f"Wecima: Skipping item due to missing link or href: {item.prettify()}")
                continue
            link = link_tag["href"]
            
            title_tag = item.select_one("strong.hasyear") or item.select_one("img")
            title = ""
            if title_tag:
                if title_tag.name == 'strong':
                    title = title_tag.get_text(strip=True)
                else: # It's an img tag
                    title = title_tag.get("alt", "N/A")
            if not title or title == "N/A":
                logger.debug(f"Wecima: Title not found or N/A for link {link} - Item HTML: {item.prettify()}")
                title = "Ø¹Ù†ÙˆØ§Ù† ØºÙŠØ± Ù…ØªÙˆÙØ±" # Provide a default title

            image_url = None
            bg_style_tag = item.select_one("span.BG--GridItem")
            if bg_style_tag and 'data-lazy-style' in bg_style_tag.attrs:
                match = re.search(r'url\((.*?)\)', bg_style_tag['data-lazy-style'])
                if match:
                    image_url = match.group(1).strip("'\"")
            
            if not image_url:
                img_tag = item.select_one("img")
                if img_tag:
                    image_url = img_tag.get("data-src") or img_tag.get("src")
            if not image_url:
                logger.debug(f"Wecima: Image URL not found for title '{title}' (link: {link}) - Item HTML: {item.prettify()}")
                image_url = "https://placehold.co/200x300/cccccc/333333?text=No+Image" # Placeholder

            movies.append({"title": title, "url": link, "image_url": image_url, "source": "Wecima"})
        except Exception as e:
            logger.error(f"âŒ Error parsing Wecima item: {e} - Item HTML causing error: {item.prettify()}")
            continue
    return movies

def parse_topcinema(soup):
    movies = []
    for item in soup.select("div.col-lg-2.col-md-3.col-sm-4.col-xs-6.col-6.MovieBlock"):
        try:
            link_tag = item.select_one("a")
            if not link_tag or not link_tag.get("href"):
                logger.debug(f"TopCinema: Skipping item due to missing link or href: {item.prettify()}")
                continue
            link = link_tag["href"]
            
            title_tag = item.select_one("h2.Title")
            title = title_tag.get_text(strip=True) if title_tag else "N/A"
            if not title or title == "N/A":
                logger.debug(f"TopCinema: Title not found or N/A for link {link} - Item HTML: {item.prettify()}")
                title = "Ø¹Ù†ÙˆØ§Ù† ØºÙŠØ± Ù…ØªÙˆÙØ±"
            
            img_tag = item.select_one("img")
            image_url = img_tag.get("data-src") or img_tag.get("src") if img_tag else None
            if not image_url:
                logger.debug(f"TopCinema: Image URL not found for title '{title}' (link: {link}) - Item HTML: {item.prettify()}")
                image_url = "https://placehold.co/200x300/cccccc/333333?text=No+Image"
            
            movies.append({"title": title, "url": link, "image_url": image_url, "source": "TopCinema"})
        except Exception as e:
            logger.error(f"âŒ Error parsing TopCinema item: {e} - Item HTML causing error: {item.prettify()}")
            continue
    return movies

def parse_cimaclub(soup):
    movies = []
    for item in soup.select("div.Small--Box"):
        try:
            link_tag = item.select_one("a.recent--block")
            if not link_tag or not link_tag.get("href"):
                logger.debug(f"CimaClub: Skipping item due to missing link or href: {item.prettify()}")
                continue
            link = link_tag["href"]
            
            title = "Ø¹Ù†ÙˆØ§Ù† ØºÙŠØ± Ù…ØªÙˆÙØ±" # Default value
            
            # Attempt 1: Get title from h2 within inner--title
            title_h2_tag = item.select_one(".inner--title h2")
            if title_h2_tag:
                extracted_title = title_h2_tag.get_text(strip=True)
                if extracted_title:
                    title = extracted_title
            
            # Attempt 2: If h2 failed, try img alt attribute
            if title == "Ø¹Ù†ÙˆØ§Ù† ØºÙŠØ± Ù…ØªÙˆÙØ±":
                img_tag_for_title = item.select_one("div.Poster img")
                if img_tag_for_title:
                    extracted_title = img_tag_for_title.get("alt", "")
                    if extracted_title:
                        title = extracted_title
            
            # Attempt 3: If img alt failed, try link title attribute
            if title == "Ø¹Ù†ÙˆØ§Ù† ØºÙŠØ± Ù…ØªÙˆÙØ±":
                extracted_title = link_tag.get("title", "")
                if extracted_title:
                    title = extracted_title

            if title == "Ø¹Ù†ÙˆØ§Ù† ØºÙŠØ± Ù…ØªÙˆÙØ±":
                logger.debug(f"CimaClub: Could not extract title for link {link} - Item HTML: {item.prettify()}")
                title = "Ø¹Ù†ÙˆØ§Ù† ØºÙŠØ± Ù…ØªÙˆÙØ±" # Ensure default if all attempts fail

            img_tag = item.select_one("div.Poster img")
            image_url = img_tag.get("data-src") or img_tag.get("src") if img_tag else None
            if not image_url:
                logger.debug(f"CimaClub: Image URL not found for title '{title}' (link: {link}) - Item HTML: {item.prettify()}")
                image_url = "https://placehold.co/200x300/cccccc/333333?text=No+Image" # Placeholder
            
            movies.append({"title": title, "url": link, "image_url": image_url, "source": "CimaClub"})
        except Exception as e:
            logger.error(f"âŒ Error parsing CimaClub item: {e} - Item HTML causing error: {item.prettify()}")
            continue
    return movies

def parse_tuktukcima(soup):
    movies = []
    for item in soup.select("div.Blocks ul li.MovieBlock"):
        try:
            link_tag = item.select_one("a")
            if not link_tag or not link_tag.get("href"):
                logger.debug(f"TukTukCima: Skipping item due to missing link or href: {item.prettify()}")
                continue
            link = link_tag["href"]
            
            title_tag = item.select_one("h2.Title")
            title = title_tag.get_text(strip=True) if title_tag else "N/A"
            if not title or title == "N/A":
                logger.debug(f"TukTukCima: Title not found or N/A for link {link} - Item HTML: {item.prettify()}")
                title = "Ø¹Ù†ÙˆØ§Ù† ØºÙŠØ± Ù…ØªÙˆÙØ±"
            
            img_tag = item.select_one("img")
            image_url = img_tag.get("data-src") or img_tag.get("src") if img_tag else None
            if not image_url:
                logger.debug(f"TukTukCima: Image URL not found for title '{title}' (link: {link}) - Item HTML: {item.prettify()}")
                image_url = "https://placehold.co/200x300/cccccc/333333?text=No+Image"
            
            movies.append({"title": title, "url": link, "image_url": image_url, "source": "TukTukCima"})
        except Exception as e:
            logger.error(f"âŒ Error parsing TukTukCima item: {e} - Item HTML causing error: {item.prettify()}")
            continue
    return movies

def parse_egy_onl(soup):
    movies = []
    for item in soup.select("div.Blocks ul.MovieList div.movie-box"):
        try:
            link_tag = item.select_one("a")
            if not link_tag or not link_tag.get("href"):
                logger.debug(f"EgyBest: Skipping item due to missing link or href: {item.prettify()}")
                continue
            link = link_tag["href"]
            
            # Ø§Ù„Ø¹Ù†ÙˆØ§Ù† Ù…ÙˆØ¬ÙˆØ¯ ÙÙŠ alt Ù„Ù„ØµÙˆØ±Ø©
            title_tag = item.select_one("img")
            title = title_tag.get("alt", "N/A") if title_tag else "N/A"
            if not title or title == "N/A":
                logger.debug(f"EgyBest: Title not found or N/A for link {link} - Item HTML: {item.prettify()}")
                title = "Ø¹Ù†ÙˆØ§Ù† ØºÙŠØ± Ù…ØªÙˆÙØ±"
            
            image_url = title_tag.get("data-src") or title_tag.get("src") if title_tag else None
            if not image_url:
                logger.debug(f"EgyBest: Image URL not found for title '{title}' (link: {link}) - Item HTML: {item.prettify()}")
                image_url = "https://placehold.co/200x300/cccccc/333333?text=No+Image"
            
            movies.append({"title": title, "url": link, "image_url": image_url, "source": "EgyBest"})
        except Exception as e:
            logger.error(f"âŒ Error parsing EgyBest item: {e} - Item HTML causing error: {item.prettify()}")
            continue
    return movies


def parse_mycima(soup):
    movies = []
    for item in soup.select("div.GridItem"):
        try:
            link_tag = item.select_one("a")
            if not link_tag or not link_tag.get("href"):
                logger.debug(f"MyCima: Skipping item due to missing link or href: {item.prettify()}")
                continue
            link = link_tag["href"]
            
            title_tag = item.select_one("strong.hasyear") or item.select_one("img")
            title = ""
            if title_tag:
                if title_tag.name == 'strong':
                    title = title_tag.get_text(strip=True)
                else: # It's an img tag
                    title = title_tag.get("alt", "N/A")
            if not title or title == "N/A":
                logger.debug(f"MyCima: Title not found or N/A for link {link} - Item HTML: {item.prettify()}")
                title = "Ø¹Ù†ÙˆØ§Ù† ØºÙŠØ± Ù…ØªÙˆÙØ±"

            image_url = None
            bg_style_tag = item.select_one("span.BG--GridItem")
            if bg_style_tag and 'data-lazy-style' in bg_style_tag.attrs:
                match = re.search(r'url\((.*?)\)', bg_style_tag['data-lazy-style'])
                if match:
                    image_url = match.group(1).strip("'\"")
            
            if not image_url:
                img_tag = item.select_one("img")
                if img_tag:
                    image_url = img_tag.get("data-src") or img_tag.get("src")
            if not image_url:
                logger.debug(f"MyCima: Image URL not found for title '{title}' (link: {link}) - Item HTML: {item.prettify()}")
                image_url = "https://placehold.co/200x300/cccccc/333333?text=No+Image"

            movies.append({"title": title, "url": link, "image_url": image_url, "source": "MyCima"})
        except Exception as e:
            logger.error(f"âŒ Error parsing MyCima item: {e} - Item HTML causing error: {item.prettify()}")
            continue
    return movies

def parse_akoam(soup):
    movies = []
    for item in soup.select("div.movie-box"):
        try:
            link_tag = item.select_one("a")
            if not link_tag or not link_tag.get("href"):
                logger.debug(f"Akoam: Skipping item due to missing link or href: {item.prettify()}")
                continue
            link = link_tag["href"]
            
            title_tag = item.select_one("h2.Title") or item.select_one("img") # Title can be in h2 or img alt
            title = ""
            if title_tag:
                if title_tag.name == 'h2':
                    title = title_tag.get_text(strip=True)
                else: # It's an img tag
                    title = title_tag.get("alt", "N/A")
            if not title or title == "N/A":
                logger.debug(f"Akoam: Title not found or N/A for link {link} - Item HTML: {item.prettify()}")
                title = "Ø¹Ù†ÙˆØ§Ù† ØºÙŠØ± Ù…ØªÙˆÙØ±"
            
            img_tag = item.select_one("img")
            image_url = img_tag.get("data-src") or img_tag.get("src") if img_tag else None
            if not image_url:
                logger.debug(f"Akoam: Image URL not found for title '{title}' (link: {link}) - Item HTML: {item.prettify()}")
                image_url = "https://placehold.co/200x300/cccccc/333333?text=No+Image"
            
            movies.append({"title": title, "url": link, "image_url": image_url, "source": "Akoam"})
        except Exception as e:
            logger.error(f"âŒ Error parsing Akoam item: {e} - Item HTML causing error: {item.prettify()}")
            continue
    return movies

def parse_shahid4u(soup):
    movies = []
    for item in soup.select("div.GridItem"):
        try:
            link_tag = item.select_one("a.MovieBlock") # Specific anchor tag
            if not link_tag or not link_tag.get("href"):
                logger.debug(f"Shahid4u: Skipping item due to missing link or href: {item.prettify()}")
                continue
            link = link_tag["href"]
            
            title_tag = item.select_one("h2.MovieTitle")
            title = title_tag.get_text(strip=True) if title_tag else "N/A"
            if not title or title == "N/A":
                logger.debug(f"Shahid4u: Title not found or N/A for link {link} - Item HTML: {item.prettify()}")
                title = "Ø¹Ù†ÙˆØ§Ù† ØºÙŠØ± Ù…ØªÙˆÙØ±"
            
            img_tag = item.select_one("img")
            image_url = img_tag.get("src") if img_tag else None # Shahid4u ÙŠØ³ØªØ®Ø¯Ù… src Ù…Ø¨Ø§Ø´Ø±Ø©
            if not image_url:
                logger.debug(f"Shahid4u: Image URL not found for title '{title}' (link: {link}) - Item HTML: {item.prettify()}")
                image_url = "https://placehold.co/200x300/cccccc/333333?text=No+Image"
            
            movies.append({"title": title, "url": link, "image_url": image_url, "source": "Shahid4u"})
        except Exception as e:
            logger.error(f"âŒ Error parsing Shahid4u item: {e} - Item HTML causing error: {item.prettify()}")
            continue
    return movies

def parse_aflamco(soup):
    movies = []
    for item in soup.select("div.ModuleItem"):
        try:
            link_tag = item.select_one("a")
            if not link_tag or not link_tag.get("href"):
                logger.debug(f"Aflamco: Skipping item due to missing link or href: {item.prettify()}")
                continue
            link = link_tag["href"]
            
            title_tag = item.select_one("h2.ModuleTitle")
            title = title_tag.get_text(strip=True) if title_tag else "N/A"
            if not title or title == "N/A":
                logger.debug(f"Aflamco: Title not found or N/A for link {link} - Item HTML: {item.prettify()}")
                title = "Ø¹Ù†ÙˆØ§Ù† ØºÙŠØ± Ù…ØªÙˆÙØ±"
            
            img_tag = item.select_one("img")
            image_url = img_tag.get("data-src") or img_tag.get("src") if img_tag else None
            if not image_url:
                logger.debug(f"Aflamco: Image URL not found for title '{title}' (link: {link}) - Item HTML: {item.prettify()}")
                image_url = "https://placehold.co/200x300/cccccc/333333?text=No+Image"
            
            movies.append({"title": title, "url": link, "image_url": image_url, "source": "Aflamco"})
        except Exception as e:
            logger.error(f"âŒ Error parsing Aflamco item: {e} - Item HTML causing error: {item.prettify()}")
            continue
    return movies

def parse_cima4u(soup):
    movies = []
    for item in soup.select("div.MovieBlock"):
        try:
            link_tag = item.select_one("a")
            if not link_tag or not link_tag.get("href"):
                logger.debug(f"Cima4u: Skipping item due to missing link or href: {item.prettify()}")
                continue
            link = link_tag["href"]
            
            title_tag = item.select_one("h2.Title")
            title = title_tag.get_text(strip=True) if title_tag else "N/A"
            if not title or title == "N/A":
                logger.debug(f"Cima4u: Title not found or N/A for link {link} - Item HTML: {item.prettify()}")
                title = "Ø¹Ù†ÙˆØ§Ù† ØºÙŠØ± Ù…ØªÙˆÙØ±"
            
            img_tag = item.select_one("img")
            image_url = img_tag.get("data-src") or img_tag.get("src") if img_tag else None
            if not image_url:
                logger.debug(f"Cima4u: Image URL not found for title '{title}' (link: {link}) - Item HTML: {item.prettify()}")
                image_url = "https://placehold.co/200x300/cccccc/333333?text=No+Image"
            
            movies.append({"title": title, "url": link, "image_url": image_url, "source": "Cima4u"})
        except Exception as e:
            logger.error(f"âŒ Error parsing Cima4u item: {e} - Item HTML causing error: {item.prettify()}")
            continue
    return movies

def parse_fushaar(soup):
    movies = []
    for item in soup.select("div.Blocks .MovieBlock"):
        try:
            link_tag = item.select_one("a")
            if not link_tag or not link_tag.get("href"):
                logger.debug(f"Fushaar: Skipping item due to missing link or href: {item.prettify()}")
                continue
            link = link_tag["href"]
            
            title_tag = item.select_one("h2.Title")
            title = title_tag.get_text(strip=True) if title_tag else "N/A"
            if not title or title == "N/A":
                logger.debug(f"Fushaar: Title not found or N/A for link {link} - Item HTML: {item.prettify()}")
                title = "Ø¹Ù†ÙˆØ§Ù† ØºÙŠØ± Ù…ØªÙˆÙØ±"
            
            img_tag = item.select_one("img")
            image_url = img_tag.get("data-lazy-src") or img_tag.get("src") if img_tag else None
            if not image_url:
                logger.debug(f"Fushaar: Image URL not found for title '{title}' (link: {link}) - Item HTML: {item.prettify()}")
                image_url = "https://placehold.co/200x300/cccccc/333333?text=No+Image"
            
            movies.append({"title": title, "url": link, "image_url": image_url, "source": "Fushaar"})
        except Exception as e:
            logger.error(f"âŒ Error parsing Fushaar item: {e} - Item HTML causing error: {item.prettify()}")
            continue
    return movies

def parse_aflaam(soup):
    movies = []
    for item in soup.select("div.movies-list-grid div.item"):
        try:
            link_tag = item.select_one("a.box")
            if not link_tag or not link_tag.get("href"):
                logger.debug(f"Aflaam: Skipping item due to missing link or href: {item.prettify()}")
                continue
            link = link_tag["href"]
            
            title_tag = item.select_one("h3.entry-title")
            title = title_tag.get_text(strip=True) if title_tag else "N/A"
            if not title or title == "N/A":
                logger.debug(f"Aflaam: Title not found or N/A for link {link} - Item HTML: {item.prettify()}")
                title = "Ø¹Ù†ÙˆØ§Ù† ØºÙŠØ± Ù…ØªÙˆÙØ±"
            
            img_tag = item.select_one("picture img.lazy") 
            image_url = img_tag.get("data-src") or img_tag.get("src") if img_tag else None
            if not image_url:
                logger.debug(f"Aflaam: Image URL not found for title '{title}' (link: {link}) - Item HTML: {item.prettify()}")
                image_url = "https://placehold.co/200x300/cccccc/333333?text=No+Image"
            
            movies.append({"title": title, "url": link, "image_url": image_url, "source": "Aflaam"})
        except Exception as e:
            logger.error(f"âŒ Error parsing Aflaam item: {e} - Item HTML causing error: {item.prettify()}")
            continue
    return movies

# --- Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ù…ÙˆØ§Ù‚Ø¹ (12 Ù…ÙˆÙ‚Ø¹) ---
# Function to get the base URL
def get_base_url(full_url):
    parsed_url = urlparse(full_url)
    return urlunparse((parsed_url.scheme, parsed_url.netloc, '', '', '', '')) + "/"

SCRAPERS = [
    {"name": "Wecima", "url": get_base_url("https://wecima.video"), "parser": parse_wecima},
    {"name": "TopCinema", "url": get_base_url("https://web6.topcinema.cam"), "parser": parse_topcinema},
    {"name": "CimaClub", "url": get_base_url("https://cimaclub.day"), "parser": parse_cimaclub},
    {"name": "TukTukCima", "url": get_base_url("https://tuktukcima.art"), "parser": parse_tuktukcima},
    {"name": "EgyBest", "url": get_base_url("https://egy.onl"), "parser": parse_egy_onl}, 
    {"name": "MyCima", "url": get_base_url("https://mycima.video"), "parser": parse_mycima},
    {"name": "Akoam", "url": get_base_url("https://akw.onl"), "parser": parse_akoam},
    {"name": "Shahid4u", "url": get_base_url("https://shahed4uapp.com"), "parser": parse_shahid4u},
    {"name": "Aflamco", "url": get_base_url("https://aflamco.cloud"), "parser": parse_aflamco},
    {"name": "Cima4u", "url": get_base_url("https://cima4u.cam"), "parser": parse_cima4u},
    {"name": "Fushaar", "url": get_base_url("https://www.fushaar.com"), "parser": parse_fushaar},
    {"name": "Aflaam", "url": get_base_url("https://aflaam.com"), "parser": parse_aflaam}
]

# --- Ø¬Ù„Ø¨ Ø§Ù„Ø£ÙÙ„Ø§Ù… Ù…Ù† Ù…ÙˆÙ‚Ø¹ ÙˆØ§Ø­Ø¯ (Ø§Ù„Ù…Ø¹Ø¯Ù„Ø© Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù… Playwright) ---
async def scrape_site_async(scraper, page): # ØªØ£Ø®Ø° page Ø¨Ø¯Ù„Ø§Ù‹ Ù…Ù† driver
    try:
        logger.info(f"Ø¬Ø§Ø±Ù ÙØ­Øµ Ù…ÙˆÙ‚Ø¹: {scraper['name']}")
        # Ø§Ù„Ø§Ù†ØªÙ‚Ø§Ù„ Ø¥Ù„Ù‰ Ø§Ù„ØµÙØ­Ø© ÙˆØ§Ù„Ø§Ù†ØªØ¸Ø§Ø± Ø­ØªÙ‰ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…Ø­ØªÙˆÙ‰
        await page.goto(scraper["url"], wait_until="domcontentloaded", timeout=60000) 

        # Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ù…Ø­ØªÙˆÙ‰ Ø§Ù„ØµÙØ­Ø© Ø¨Ø¹Ø¯ ØªØ­Ù…ÙŠÙ„Ù‡Ø§ Ø¨Ø§Ù„ÙƒØ§Ù…Ù„
        page_content = await page.content()

        # Ø­ÙØ¸ Ù†Ø³Ø®Ø© Ù…Ù† HTML Ø§Ù„ØµÙØ­Ø© Ù„Ù„Ù…Ø¹Ø§ÙŠÙ†Ø© (Ù„Ù„ØªØµØ­ÙŠØ­)
        with open(f"debug_{scraper['name']}.html", "wb") as f:
            f.write(page_content.encode('utf-8')) # ØªØ£ÙƒØ¯ Ù…Ù† Ø§Ù„ØªØ±Ù…ÙŠØ²

        soup = BeautifulSoup(page_content, 'html.parser')
        movies = scraper["parser"](soup)

        # Ø·Ø¨Ø§Ø¹Ø© Ø¹Ø¯Ø¯ Ø§Ù„Ø£ÙÙ„Ø§Ù… Ø§Ù„Ù…Ø³ØªØ®Ø±Ø¬Ø©
        if movies:
            logger.info(f"âœ… {len(movies)} ÙÙŠÙ„Ù… ØªÙ… Ø§Ø³ØªØ®Ø±Ø§Ø¬Ù‡ Ù…Ù† {scraper['name']}")
        else:
            logger.warning(f"âš ï¸ Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø£ÙÙ„Ø§Ù… ÙÙŠ {scraper['name']} Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù…Ø­Ø¯Ø¯Ø§Øª Ø§Ù„Ø­Ø§Ù„ÙŠØ©. ÙŠØ±Ø¬Ù‰ Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† debug_{scraper['name']}.html")

        return movies

    except Exception as e:
        logger.error(f"âŒ Ø®Ø·Ø£ ØºÙŠØ± Ù…ØªÙˆÙ‚Ø¹ Ø£Ø«Ù†Ø§Ø¡ ØªØ­Ù„ÙŠÙ„ {scraper['name']}: {e}")
        return []

# --- Ø¬Ù„Ø¨ Ø§Ù„Ø£ÙÙ„Ø§Ù… Ù…Ù† Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…ÙˆØ§Ù‚Ø¹ (Ø§Ù„Ù…Ø¹Ø¯Ù„Ø© Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù… Playwright) ---
async def scrape_movies_async(): # ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ø¯Ø§Ù„Ø© Ù„ØªØµØ¨Ø­ async
    new_movies = []
    total_added_count = 0 
    browser = None # ØªÙ‡ÙŠØ¦Ø© browser Ø®Ø§Ø±Ø¬ try Ù„ØªØ£ÙƒÙŠØ¯ Ø¥ØºÙ„Ø§Ù‚Ù‡ ÙÙŠ finally
    try:
        # ØªØ¹ÙŠÙŠÙ† Ù…Ø³Ø§Ø± Ø§Ù„Ù…ØªØµÙØ­Ø§Øª Ù„Ù€ Playwright
        # Ù‡Ø°Ø§ ÙŠØ®Ø¨Ø± Playwright Ø¨Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„Ù…ØªØµÙØ­Ø§Øª ÙÙŠ Ø¯Ù„ÙŠÙ„ Ø°Ø§ÙƒØ±Ø© Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª Ù„Ù€ Replit
        os.environ["PLAYWRIGHT_BROWSERS_PATH"] = os.path.join(os.path.expanduser("~"), ".cache", "ms-playwright")
        
        async with async_playwright() as p:
            # ØªØ´ØºÙŠÙ„ Ù…ØªØµÙØ­ Chromium ÙÙŠ Ø§Ù„ÙˆØ¶Ø¹ Ø§Ù„Ù…Ø®ÙÙŠ
            browser = await p.chromium.launch(
                headless=True,
                args=["--no-sandbox", "--disable-gpu", "--disable-dev-shm-usage"]
            ) 

            conn = sqlite3.connect('movies.db')
            c = conn.cursor()

            # Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù‡Ø§Ù… ÙƒØ´Ø· Ù„ÙƒÙ„ Ù…ÙˆÙ‚Ø¹ Ø¨Ø§Ù„ØªÙˆØ§Ø²ÙŠ
            tasks = []
            for scraper in SCRAPERS:
                page = await browser.new_page() # Ø¥Ù†Ø´Ø§Ø¡ ØµÙØ­Ø© Ø¬Ø¯ÙŠØ¯Ø© Ù„ÙƒÙ„ Ù…ÙˆÙ‚Ø¹
                tasks.append(scrape_site_async(scraper, page))

            # ØªÙ†ÙÙŠØ° Ø¬Ù…ÙŠØ¹ Ù…Ù‡Ø§Ù… Ø§Ù„ÙƒØ´Ø· Ø¨Ø§Ù„ØªÙˆØ§Ø²ÙŠ
            results = await asyncio.gather(*tasks)

            for scraper_idx, movies in enumerate(results):
                scraper = SCRAPERS[scraper_idx] # Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ø³ÙƒØ±Ø§Ø¨Ø± Ø§Ù„Ø£ØµÙ„ÙŠØ©
                added_count = 0
                for movie in movies:
                    try:
                        # ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ø¹Ù†ÙˆØ§Ù† Ù‚Ø¨Ù„ Ø¥Ø¯Ø®Ø§Ù„Ù‡ ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
                        clean_title_text = clean_title(movie["title"])
                        # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù…Ø§ Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„ÙÙŠÙ„Ù… Ù…ÙˆØ¬ÙˆØ¯Ù‹Ø§ Ø¨Ø§Ù„ÙØ¹Ù„ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø±Ø§Ø¨Ø· Ø§Ù„Ù†Ø¸ÙŠÙ (clean URL)
                        c.execute("SELECT id FROM movies WHERE url = ?", (movie["url"],))
                        if c.fetchone() is None:
                            c.execute("INSERT INTO movies (title, url, source, image_url) VALUES (?, ?, ?, ?)",
                                      (clean_title_text, movie["url"], scraper["name"], movie.get("image_url")))
                            new_movies.append({
                                "title": clean_title_text,
                                "url": movie["url"],
                                "source": scraper["name"],
                                "image_url": movie.get("image_url")
                            })
                            added_count += 1
                    except sqlite3.IntegrityError:
                        # Ù‡Ø°Ø§ ÙŠØ­Ø¯Ø« Ø¥Ø°Ø§ ÙƒØ§Ù† Ù‡Ù†Ø§Ùƒ ÙÙŠÙ„Ù… Ø¨Ù†ÙØ³ Ø§Ù„Ù€ URL Ù…ÙˆØ¬ÙˆØ¯ Ø¨Ø§Ù„ÙØ¹Ù„ (UNIQUE constraint)
                        pass
                    except Exception as e:
                        logger.error(f"  âŒ Ø®Ø·Ø£ ÙÙŠ Ø¥Ø¶Ø§ÙØ© ÙÙŠÙ„Ù… Ù…Ù† {scraper['name']} ({movie.get('title', 'N/A')}): {e}")
                
                if added_count > 0:
                    logger.info(f"  âœ… ØªÙ…Øª Ø¥Ø¶Ø§ÙØ© {added_count} Ø£ÙÙ„Ø§Ù… Ø¬Ø¯ÙŠØ¯Ø© Ù…Ù† {scraper['name']}")
                total_added_count += added_count
                conn.commit()

            conn.close()

    except Exception as e:
        logger.critical(f"âš ï¸ Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ Ø¬Ù…Ø¹ Ø§Ù„Ø£ÙÙ„Ø§Ù…: {e}") 
    finally:
        if browser:
            await browser.close() # ØªØ£ÙƒØ¯ Ù…Ù† Ø¥ØºÙ„Ø§Ù‚ Ø§Ù„Ù…ØªØµÙØ­
            logger.info("Ù…ØªØµÙØ­ Playwright ØªÙ… Ø¥ØºÙ„Ø§Ù‚Ù‡.")
            
    logger.info(f"âœ… ØªÙ…Øª Ø¥Ø¶Ø§ÙØ© {total_added_count} ÙÙŠÙ„Ù… Ø¬Ø¯ÙŠØ¯ ÙÙŠ Ù‡Ø°Ù‡ Ø§Ù„Ø¬ÙˆÙ„Ø©.") 
    return new_movies

# --- Ø¥Ø±Ø³Ø§Ù„ Ø§Ù„Ø£ÙÙ„Ø§Ù… Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø© Ù„Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ† ---
async def send_new_movies(context: ContextTypes.DEFAULT_TYPE):
    # Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ Ø§Ù„Ø¯Ø§Ù„Ø© ØºÙŠØ± Ø§Ù„Ù…ØªØ²Ø§Ù…Ù†Ø© Ù„ÙƒØ´Ø· Ø§Ù„Ø£ÙÙ„Ø§Ù…
    new_movies = await scrape_movies_async() 
    if not new_movies:
        logger.info("Ù„Ø§ ØªÙˆØ¬Ø¯ Ø£ÙÙ„Ø§Ù… Ø¬Ø¯ÙŠØ¯Ø© Ù„Ù„Ø¥Ø±Ø³Ø§Ù„.")
        return

    conn = sqlite3.connect('movies.db')
    c = conn.cursor()
    c.execute("SELECT user_id FROM users")
    users = c.fetchall()
    conn.close()

    # ØªØ¬Ù…ÙŠØ¹ Ø§Ù„Ø£ÙÙ„Ø§Ù… Ø­Ø³Ø¨ Ø§Ù„Ù…ØµØ¯Ø±
    movies_by_source = {}
    for movie in new_movies:
        if movie['source'] not in movies_by_source:
            movies_by_source[movie['source']] = []
        movies_by_source[movie['source']].append(movie)

    for user_id, in users:
        try:
            message_parts = []
            message_parts.append("ğŸ¬ <b>Ø£ÙÙ„Ø§Ù… Ø¬Ø¯ÙŠØ¯Ø© Ù…ØªØ§Ø­Ø©:</b>\n\n")
            
            for source, movies in movies_by_source.items():
                message_parts.append(f"<b>{source}:</b>\n")
                # Ø¹Ø±Ø¶ Ø£ÙˆÙ„ 5 Ø£ÙÙ„Ø§Ù… Ù…Ù† ÙƒÙ„ Ù…ØµØ¯Ø±
                for movie in movies[:5]: 
                    # Ø¯Ù…Ø¬ Ø±Ø§Ø¨Ø· Ø§Ù„ØµÙˆØ±Ø© ÙƒÙ†Øµ Ø¨Ø¬Ø§Ù†Ø¨ Ø±Ø§Ø¨Ø· Ø§Ù„ÙÙŠÙ„Ù…
                    image_link_text = f" (<a href='{movie['image_url']}'>ØµÙˆØ±Ø©</a>)" if movie.get('image_url') else ""
                    message_parts.append(f"â€¢ <a href='{movie['url']}'>{movie['title']}</a>{image_link_text}\n")
                message_parts.append("\n")
            
            final_message = "".join(message_parts)

            await context.bot.send_message(
                chat_id=user_id,
                text=final_message,
                parse_mode='HTML',
                disable_web_page_preview=True # Keep this true to prevent large URL previews
            )
            await asyncio.sleep(0.3) # ØªØ£Ø®ÙŠØ± Ø¨Ø³ÙŠØ· Ù„ØªØ¬Ù†Ø¨ Ø­Ø¯ÙˆØ¯ Ù…Ø¹Ø¯Ù„ Telegram API
        except Exception as e:
            logger.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø¥Ø±Ø³Ø§Ù„ Ø§Ù„Ø£ÙÙ„Ø§Ù… Ù„Ù„Ù…Ø³ØªØ®Ø¯Ù… {user_id}: {e}")

# --- Ø£Ù…Ø± Ø¨Ø¯Ø¡ Ø§Ù„Ø¨ÙˆØª ---
async def start(update: Update, context: ContextTypes.DEFAULT_TYPE):
    user = update.effective_user
    add_user(user.id, user.username, user.first_name, user.last_name)

    welcome_msg = (
        f"ğŸ‰ Ù…Ø±Ø­Ø¨Ø§Ù‹ {user.first_name}!\n"
        "Ø£Ù†Ø§ Ø¨ÙˆØª Ø§Ù„Ø£ÙÙ„Ø§Ù… Ø§Ù„Ø°ÙƒÙŠØŒ Ø³Ø£Ø±Ø³Ù„ Ù„Ùƒ Ø£Ø­Ø¯Ø« Ø§Ù„Ø£ÙÙ„Ø§Ù… ØªÙ„Ù‚Ø§Ø¦ÙŠØ§Ù‹ Ù…Ù† 12 Ù…ÙˆÙ‚Ø¹ Ø³ÙŠÙ†Ù…Ø§Ø¦ÙŠ Ø´Ù‡ÙŠØ±.\n\n"
        "ğŸ“º <b>Ø§Ù„Ù…ÙˆØ§Ù‚Ø¹ Ø§Ù„Ù…Ø¯Ø¹ÙˆÙ…Ø©:</b>\n"
        "- Wecima, TopCinema, CimaClub\n"
        "- TukTukCima, EgyBest, MyCima\n"
        "- Akoam, Shahid4u, Aflamco\n"
        "- Cima4u, Fushaar, Aflaam\n\n"
        "â° Ø³ÙŠØµÙ„Ùƒ ØªØ­Ø¯ÙŠØ« Ø¨Ø§Ù„Ø£ÙÙ„Ø§Ù… Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø© ÙƒÙ„ Ø³Ø§Ø¹Ø© ØªÙ„Ù‚Ø§Ø¦ÙŠØ§Ù‹\n"
        "Ù„Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ ØªØ­Ø¯ÙŠØ« ÙŠØ¯ÙˆÙŠØŒ Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø£Ù…Ø± /update"
    )
    
    await update.message.reply_text(
        welcome_msg,
        parse_mode='HTML'
    )

# --- Ø£Ù…Ø± ÙØ­Øµ Ø­Ø§Ù„Ø© Ø§Ù„Ø¨ÙˆØª ---
async def alive(update: Update, context: ContextTypes.DEFAULT_TYPE):
    conn = sqlite3.connect('movies.db')
    c = conn.cursor()
    c.execute("SELECT COUNT(*) FROM movies")
    movies_count = c.fetchone()[0]
    c.execute("SELECT COUNT(*) FROM users")
    users_count = c.fetchone()[0]
    conn.close()
    
    status_msg = (
        "âœ… Ø£Ù†Ø§ Ø´ØºØ§Ù„ ÙˆÙ‚ÙˆÙŠ!\n\n"
        f"ğŸ¥ Ø¹Ø¯Ø¯ Ø§Ù„Ø£ÙÙ„Ø§Ù… ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª: <b>{movies_count}</b>\n"
        f"ğŸ‘¥ Ø¹Ø¯Ø¯ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ†: <b>{users_count}</b>\n"
        "â±ï¸ Ø¢Ø®Ø± ØªØ­Ø¯ÙŠØ«: Ù…Ù†Ø° Ù‚Ù„ÙŠÙ„\n"
        "ğŸ”„ Ø§Ù„ØªØ­Ø¯ÙŠØ« Ø§Ù„ØªØ§Ù„ÙŠ: Ø®Ù„Ø§Ù„ Ø³Ø§Ø¹Ø©"
    )
    
    await update.message.reply_text(
        status_msg,
        parse_mode='HTML'
    )

# --- Ø£Ù…Ø± Ø§Ù„ØªØ­Ø¯ÙŠØ« Ø§Ù„ÙŠØ¯ÙˆÙŠ ---
async def manual_update(update: Update, context: ContextTypes.DEFAULT_TYPE):
    user = update.effective_user
    await update.message.reply_text("â³ Ø¬Ø§Ø±Ù Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø£ÙÙ„Ø§Ù… Ø¬Ø¯ÙŠØ¯Ø©... Ù‚Ø¯ ÙŠØ³ØªØºØ±Ù‚ Ù‡Ø°Ø§ Ø¨Ø¹Ø¶ Ø§Ù„ÙˆÙ‚Øª.")
    
    new_movies = await scrape_movies_async() # ØªÙ… ØªØ¹Ø¯ÙŠÙ„ Ø§Ù„Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ Ù„ÙŠÙƒÙˆÙ† async
    if not new_movies:
        await update.message.reply_text("âš ï¸ Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø£ÙÙ„Ø§Ù… Ø¬Ø¯ÙŠØ¯Ø© ÙÙŠ Ù‡Ø°Ù‡ Ø§Ù„Ø¬ÙˆÙ„Ø©.")
        return
    
    message_parts = []
    message_parts.append("ğŸ‰ <b>ØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø£ÙÙ„Ø§Ù… Ø¬Ø¯ÙŠØ¯Ø©:</b>\n\n")

    movies_by_source = {}
    for movie in new_movies:
        if movie['source'] not in movies_by_source:
            movies_by_source[movie['source']] = []
        movies_by_source[movie['source']].append(movie)

    for source, movies in movies_by_source.items():
        message_parts.append(f"<b>{source}:</b>\n")
        for movie in movies[:5]: # Ø¹Ø±Ø¶ Ø£ÙˆÙ„ 5 Ø£ÙÙ„Ø§Ù… Ø¬Ø¯ÙŠØ¯Ø© Ù…Ù† ÙƒÙ„ Ù…ØµØ¯Ø±
            image_link_text = f" (<a href='{movie['image_url']}'>ØµÙˆØ±Ø©</a>)" if movie.get('image_url') else ""
            message_parts.append(f"â€¢ <a href='{movie['url']}'>{movie['title']}</a>{image_link_text}\n")
        message_parts.append("\n")
    
    final_message = "".join(message_parts)

    await update.message.reply_text(
        final_message,
        parse_mode='HTML',
        disable_web_page_preview=True
    )

# --- Ø¬Ø¯ÙˆÙ„Ø© Ø§Ù„Ù…Ù‡Ø§Ù… ---
def schedule_job(application):
    loop = asyncio.new_event_loop()
    asyncio.set_event_loop(loop)

    def run_async_task_wrapper():
        try:
            # ØªØ´ØºÙŠÙ„ Ø§Ù„Ø¯Ø§Ù„Ø© ØºÙŠØ± Ø§Ù„Ù…ØªØ²Ø§Ù…Ù†Ø© send_new_movies
            loop.run_until_complete(send_new_movies(application))
        except Exception as e:
            logger.error(f"Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ù…Ù‡Ù…Ø© Ø§Ù„Ù…Ø¬Ø¯ÙˆÙ„Ø©: {e}")

    schedule.every(1).hours.do(run_async_task_wrapper)
    
    logger.info("Ø¨Ø¯Ø¡ Ø¹Ù…Ù„ÙŠØ© Ø¬Ù…Ø¹ Ø§Ù„Ø£ÙÙ„Ø§Ù… Ø§Ù„Ø£ÙˆÙ„ÙŠØ©...")
    run_async_task_wrapper()  

    while True:
        schedule.run_pending()
        time.sleep(30)

# --- ØªØ´ØºÙŠÙ„ Ø§Ù„Ø¨ÙˆØª ---
def main():
    init_db()
    logger.info("ØªÙ… ØªÙ‡ÙŠØ¦Ø© Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª")

    application = Application.builder().token(TOKEN).build()
    
    application.add_handler(CommandHandler("start", start))
    application.add_handler(CommandHandler("alive", alive))
    application.add_handler(CommandHandler("update", manual_update))

    threading.Thread(target=schedule_job, args=(application,), daemon=True).start()

    logger.info("âœ… Ø§Ù„Ø¨ÙˆØª ÙŠØ¹Ù…Ù„ Ø§Ù„Ø¢Ù† Ù…Ø¹ 12 Ù…ÙˆÙ‚Ø¹ Ø³ÙŠÙ†Ù…Ø§Ø¦ÙŠ")
    logger.info("â±ï¸ ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø£ÙÙ„Ø§Ù… ÙƒÙ„ Ø³Ø§Ø¹Ø© ØªÙ„Ù‚Ø§Ø¦ÙŠØ§Ù‹")
    logger.info("ï¿½ Ø®Ø§Ø¯Ù… Keep-Alive ÙŠØ¹Ù…Ù„ Ø¹Ù„Ù‰ Ø§Ù„Ù…Ù†ÙØ° 8080")
    logger.info("ğŸ”„ Ø§Ø³ØªØ®Ø¯Ù… /update Ù„ØªØ­Ø¯ÙŠØ« ÙŠØ¯ÙˆÙŠ")
    application.run_polling()

if __name__ == '__main__':
    main()

