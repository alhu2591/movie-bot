import subprocess
import sys
import os
import re
import sqlite3
import threading
import time
import asyncio
import requests
from bs4 import BeautifulSoup
from flask import Flask
from urllib.parse import urlparse, urlunparse
import schedule
from datetime import datetime, timedelta
import html
import logging

# Import necessary Telegram types for permanent keyboard
from telegram import Update, InlineKeyboardButton, InlineKeyboardMarkup, ReplyKeyboardMarkup, KeyboardButton
from telegram.ext import Application, CommandHandler, ContextTypes, CallbackQueryHandler, MessageHandler, filters


# --- Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„ØªØ³Ø¬ÙŠÙ„ (Logging) ---
logging.basicConfig(
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    level=logging.INFO
)
logger = logging.getLogger(__name__)

# --- Aggressive Package Installation and Verification ---
def ensure_packages_installed():
    required_pip_packages = [
        "requests", "beautifulsoup4", "lxml", "python-telegram-bot"
    ]
    
    # Aggressive uninstall to clear any conflicting packages
    logger.info("Attempting aggressive uninstallation of potentially conflicting packages...")
    for pkg in ["telegram", "python-telegram-bot", "requests", "beautifulsoup4", "lxml"]:
        try:
            result = subprocess.run([sys.executable, "-m", "pip", "uninstall", "-y", pkg], capture_output=True, text=True)
            if result.returncode == 0:
                logger.info(f"Uninstalled '{pkg}' (if present).")
            else:
                logger.debug(f"Uninstall of '{pkg}' returned non-zero ({result.returncode}), stderr: {result.stderr.strip()}")
        except Exception as e:
            logger.warning(f"Error during '{pkg}' uninstallation: {e}")

    # Install/reinstall all required packages
    try:
        install_cmd = [sys.executable, "-m", "pip", "install", "--upgrade", "--force-reinstall"] + required_pip_packages
        logger.info(f"Running pip install command: {' '.join(install_cmd)}")
        result = subprocess.run(install_cmd, capture_output=True, text=True, check=True)
        logger.info("Required packages installed/reinstalled successfully.")
        logger.debug(f"pip install stdout: {result.stdout}")
    except subprocess.CalledProcessError as e:
        logger.critical(f"âŒ Failed to install Python packages. Error: {e.stderr}")
        logger.critical("Please check your internet connection and Replit environment settings.")
        sys.exit(1)
    except Exception as e:
        logger.critical(f"âŒ Unexpected error during Python package installation: {e}")
        sys.exit(1)

    # After mass installation/reinstallation, specifically verify telegram imports
    logger.info("Verifying critical imports...")
    try:
        from telegram import Update, InlineKeyboardButton, InlineKeyboardMarkup, ReplyKeyboardMarkup, KeyboardButton
        from telegram.ext import Application, CommandHandler, ContextTypes, CallbackQueryHandler, MessageHandler, filters
        logger.info("âœ… Core Python-Telegram-Bot imports successful.")
    except ImportError as e:
        logger.critical(f"âŒ Critical ImportError after package installation: {e}")
        logger.critical("This usually means 'python-telegram-bot' is not correctly installed or a conflicting 'telegram' package exists.")
        logger.critical("Please try running 'pip uninstall telegram python-telegram-bot' then 'pip install python-telegram-bot' manually in your Replit shell, and restart the Repl.")
        sys.exit(1) # Exit if critical imports fail even after reinstallation

# Call this at the very beginning of the script execution
ensure_packages_installed()


# --- Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ø¨ÙˆØª ---
TOKEN = "7576844775:AAHdO2WNOetUhty_RlADiTi4QhyNXZnM2Ds" 
ADMIN_CHAT_ID = os.getenv("ADMIN_CHAT_ID") # Ù…ØªØºÙŠØ± Ø¨ÙŠØ¦Ø© Ù„Ù€ ID Ø§Ù„Ù…Ø´Ø±Ù

# --- Ø¥Ø¹Ø¯Ø§Ø¯ Ø®Ø§Ø¯Ù… keep_alive ---
app = Flask(__name__)
@app.route('/')
def home():
    return "ğŸ¬ Ø¨ÙˆØª Ø§Ù„Ø£ÙÙ„Ø§Ù… ÙŠØ¹Ù…Ù„ Ø¨Ù†Ø¬Ø§Ø­! | 12 Ù…ÙˆÙ‚Ø¹ Ø³ÙŠÙ†Ù…Ø§Ø¦ÙŠ | ØªØ­Ø¯ÙŠØ« ÙƒÙ„ 6 Ø³Ø§Ø¹Ø§Øª | Keep-Alive Ù…ÙØ¹Ù„" # ØªÙ… ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø±Ø³Ø§Ù„Ø© Ù„ØªØ¹ÙƒØ³ 6 Ø³Ø§Ø¹Ø§Øª Ùˆ Keep-Alive

def run_flask_app():
    app.run(host='0.0.0.0', port=8080)

# Ø¨Ø¯Ø¡ Flask ÙÙŠ Ù…Ø¤Ø´Ø± ØªØ±Ø§Ø¨Ø· Ù…Ù†ÙØµÙ„
threading.Thread(target=run_flask_app, daemon=True).start()

# --- ØªÙ‡ÙŠØ¦Ø© Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ---
def init_db():
    conn = sqlite3.connect('movies.db')
    c = conn.cursor()
    # Ø¬Ø¯ÙˆÙ„ Ø§Ù„Ø£ÙÙ„Ø§Ù…
    c.execute('''CREATE TABLE IF NOT EXISTS movies
                (id INTEGER PRIMARY KEY AUTOINCREMENT,
                 title TEXT NOT NULL,
                 url TEXT NOT NULL UNIQUE,
                 source TEXT NOT NULL,
                 image_url TEXT,
                 category TEXT,
                 description TEXT,
                 release_year INTEGER,
                 last_updated TIMESTAMP DEFAULT CURRENT_TIMESTAMP)''')
    
    # Ù…Ø­Ø§ÙˆÙ„Ø© Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© Ø¥Ø°Ø§ Ù„Ù… ØªÙƒÙ† Ù…ÙˆØ¬ÙˆØ¯Ø©
    try:
        c.execute("ALTER TABLE movies ADD COLUMN category TEXT")
    except sqlite3.OperationalError as e:
        if "duplicate column name" not in str(e):
            logger.error(f"Error altering movies table to add category column: {e}")

    try:
        c.execute("ALTER TABLE movies ADD COLUMN image_url TEXT")
    except sqlite3.OperationalError as e:
        if "duplicate column name" not in str(e):
            logger.error(f"Error altering movies table to add image_url column: {e}")
            
    try:
        c.execute("ALTER TABLE movies ADD COLUMN description TEXT")
    except sqlite3.OperationalError as e:
        if "duplicate column name" not in str(e):
            logger.error(f"Error altering movies table to add description column: {e}")
            
    try:
        c.execute("ALTER TABLE movies ADD COLUMN release_year INTEGER")
    except sqlite3.OperationalError as e:
        if "duplicate column name" not in str(e):
            logger.error(f"Error altering movies table to add release_year column: {e}")
    
    # Ø¬Ø¯ÙˆÙ„ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ† Ù…Ø¹ ØªÙØ¶ÙŠÙ„Ø§Øª Ø§Ù„ØªÙ†Ø¨ÙŠÙ‡Ø§Øª
    c.execute('''CREATE TABLE IF NOT EXISTS users
                (user_id INTEGER PRIMARY KEY,
                 username TEXT,
                 first_name TEXT,
                 last_name TEXT,
                 join_date TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                 receive_movies INTEGER DEFAULT 1,    -- 1 for true, 0 for false
                 receive_series INTEGER DEFAULT 1,
                 receive_anime INTEGER DEFAULT 1)''')
    
    # Ù…Ø­Ø§ÙˆÙ„Ø© Ø¥Ø¶Ø§ÙØ© Ø£Ø¹Ù…Ø¯Ø© Ø§Ù„ØªÙØ¶ÙŠÙ„Ø§Øª Ø¥Ø°Ø§ Ù„Ù… ØªÙƒÙ† Ù…ÙˆØ¬ÙˆØ¯Ø©
    for col in ['receive_movies', 'receive_series', 'receive_anime']:
        try:
            c.execute(f"ALTER TABLE users ADD COLUMN {col} INTEGER DEFAULT 1")
        except sqlite3.OperationalError as e:
            if "duplicate column name" not in str(e):
                logger.error(f"Error altering users table to add {col} column: {e}")

    # Ø¬Ø¯ÙˆÙ„ Ø­Ø§Ù„Ø© Ø§Ù„Ù…ÙˆØ§Ù‚Ø¹
    c.execute('''CREATE TABLE IF NOT EXISTS site_status
                (site_name TEXT PRIMARY KEY,
                 last_scraped TIMESTAMP,
                 status TEXT DEFAULT 'unknown')''') # 'active', 'failed', 'unknown'

    conn.commit()
    conn.close()
    logger.info("ØªÙ… ØªÙ‡ÙŠØ¦Ø© Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª.")

# --- Ø¥Ø¶Ø§ÙØ© Ù…Ø³ØªØ®Ø¯Ù… Ø¬Ø¯ÙŠØ¯ ---
def add_user(user_id, username, first_name, last_name):
    conn = sqlite3.connect('movies.db')
    c = conn.cursor()
    c.execute("INSERT OR IGNORE INTO users (user_id, username, first_name, last_name, receive_movies, receive_series, receive_anime) VALUES (?, ?, ?, ?, 1, 1, 1)",
              (user_id, username, first_name, last_name))
    conn.commit()
    conn.close()
    logger.info(f"ØªÙ… Ø¥Ø¶Ø§ÙØ©/ØªØ­Ø¯ÙŠØ« Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…: {user_id}")

# --- ØªØ­Ø¯ÙŠØ« ØªÙØ¶ÙŠÙ„Ø§Øª Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… ---
def update_user_preference(user_id, preference_type, value):
    conn = sqlite3.connect('movies.db')
    c = conn.cursor()
    try:
        c.execute(f"UPDATE users SET {preference_type} = ? WHERE user_id = ?", (value, user_id))
        conn.commit()
        logger.info(f"ØªÙ… ØªØ­Ø¯ÙŠØ« ØªÙØ¶ÙŠÙ„ {preference_type} Ù„Ù„Ù…Ø³ØªØ®Ø¯Ù… {user_id} Ø¥Ù„Ù‰ {value}")
        return True
    except Exception as e:
        logger.error(f"Ø®Ø·Ø£ ÙÙŠ ØªØ­Ø¯ÙŠØ« ØªÙØ¶ÙŠÙ„ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… {user_id} Ù„Ù€ {preference_type}: {e}")
        return False
    finally:
        conn.close()

# --- Ø¬Ù„Ø¨ ØªÙØ¶ÙŠÙ„Ø§Øª Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… ---
def get_user_preferences(user_id):
    conn = sqlite3.connect('movies.db')
    c = conn.cursor()
    c.execute("SELECT receive_movies, receive_series, receive_anime FROM users WHERE user_id = ?", (user_id,))
    prefs = c.fetchone()
    conn.close()
    if prefs:
        return {"movies": bool(prefs[0]), "series": bool(prefs[1]), "anime": bool(prefs[2])}
    return {"movies": True, "series": True, "anime": True} # ØªÙØ¶ÙŠÙ„Ø§Øª Ø§ÙØªØ±Ø§Ø¶ÙŠØ©


# --- ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ø¹Ù†Ø§ÙˆÙŠÙ† ---
def clean_title(title):
    # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ø£Ù‚ÙˆØ§Ø³ ÙˆØ§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ø²Ø§Ø¦Ø¯Ø© Ù…Ø«Ù„ "Ù…ØªØ±Ø¬Ù…" Ùˆ "HD"
    title = re.sub(r'\s*\(\d{4}\)|\s*\[.*?\]|\s*Ù…ØªØ±Ø¬Ù…|\s*Ø§ÙˆÙ† Ù„Ø§ÙŠÙ†|\s*online|\s*HD|\s*WEB-DL|\s*BluRay|\s*Ù†Ø³Ø®Ø© Ù…Ø¯Ø¨Ù„Ø¬Ø©', '', title, flags=re.IGNORECASE)
    # Ø¥Ø²Ø§Ù„Ø© Ø£ÙŠ Ø£Ø­Ø±Ù ØºÙŠØ± Ø§Ù„Ø£Ø¨Ø¬Ø¯ÙŠØ© Ø§Ù„Ø±Ù‚Ù…ÙŠØ© ÙˆØ§Ù„Ù…Ø³Ø§ÙØ§Øª ÙˆØ§Ù„Ø­Ø±ÙˆÙ Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©
    title = re.sub(r'[^\w\s\u0600-\u06FF]+', '', title)
    # Ø§Ø³ØªØ¨Ø¯Ø§Ù„ Ø§Ù„Ù…Ø³Ø§ÙØ§Øª Ø§Ù„Ù…ØªØ¹Ø¯Ø¯Ø© Ø¨Ù…Ø³Ø§ÙØ© ÙˆØ§Ø­Ø¯Ø©
    title = re.sub(r'\s{2,}', ' ', title)
    return title.strip()

# --- Ø¯Ø§Ù„Ø© Ù…Ø³Ø§Ø¹Ø¯Ø© Ù„ØªØ­Ø¯ÙŠØ¯ Ø§Ù„ÙØ¦Ø© ---
def deduce_category(title, url, category_hint=None):
    if category_hint and category_hint != "mixed":
        return category_hint

    title_lower = title.lower()
    url_lower = url.lower()
    
    if "Ù…Ø³Ù„Ø³Ù„" in title_lower or "series" in url_lower or "Ù…Ø³Ù„Ø³Ù„Ø§Øª" in url_lower or "/series" in url_lower or "/tv" in url_lower or "Ù…Ø³Ù„Ø³Ù„Ø§Øª-Ø§Ø¬Ù†Ø¨ÙŠ" in url_lower:
        return "Ù…Ø³Ù„Ø³Ù„"
    if "Ø§Ù†Ù…ÙŠ" in title_lower or "anime" in url_lower or "Ø£Ù†Ù…ÙŠ" in title_lower:
        return "Ø£Ù†Ù…ÙŠ"
    return "ÙÙŠÙ„Ù…" # Ø§Ù„ÙØ¦Ø© Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ©

# --- Ø¯Ø§Ù„Ø© Ù…Ø³Ø§Ø¹Ø¯Ø© Ù„Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ÙˆØµÙ Ø§Ù„Ù…ÙØµÙ„ ÙˆØ³Ù†Ø© Ø§Ù„Ø¥ØµØ¯Ø§Ø± Ù…Ù† ØµÙØ­Ø© Ø§Ù„ÙÙŠÙ„Ù… Ø§Ù„ÙØ±Ø¯ÙŠØ© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Requests ---
def extract_detailed_movie_info_requests(movie_url: str, movie_title_for_ref: str = "") -> (str, int | None):
    description = ""
    release_year = None
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/125.0.0.0 Safari/537.36'
        }
        response = requests.get(movie_url, headers=headers, timeout=30)
        response.raise_for_status() # Raise HTTPError for bad responses (4xx or 5xx)
        
        try:
            movie_soup = BeautifulSoup(response.content, 'lxml')
        except Exception as bs_e:
            logger.warning(f"LXML parser not available for {movie_url}, falling back to html.parser: {bs_e}")
            movie_soup = BeautifulSoup(response.content, 'html.parser')

        # 1. Try to get description from meta tag
        meta_description = movie_soup.find('meta', attrs={'name': 'description'})
        if meta_description and meta_description.get('content'):
            description = meta_description['content'].strip()
            if len(description) < 100 or "Ù…Ø´Ø§Ù‡Ø¯Ø© ÙˆØªØ­Ù…ÙŠÙ„" in description:
                description = "" 
        
        # 2. If meta tag failed, try common description selectors
        if not description:
            for selector in [
                "div.story", "div.Description", "div.single-text", "div#plot",
                "p.movie-description", "div.entry-content p", "div.BlockItemFull p",
                "div.post-story p", "div.MovieContent__Details__Story" 
            ]:
                tag = movie_soup.select_one(selector)
                if tag:
                    text = tag.get_text(separator=' ', strip=True) 
                    if len(text) > 50 and len(text) < 5000: 
                        description = text
                        break
        
        # 3. Clean the description from promotional phrases
        if description:
            promo_phrases = [
                r'Ù…Ø´Ø§Ù‡Ø¯Ø© ÙˆØªØ­Ù…ÙŠÙ„ (ÙÙŠÙ„Ù…|Ù…Ø³Ù„Ø³Ù„|Ø§Ù†Ù…ÙŠ)?\s*', r'Ù…Ø´Ø§Ù‡Ø¯Ø© (ÙÙŠÙ„Ù…|Ù…Ø³Ù„Ø³Ù„|Ø§Ù†Ù…ÙŠ)?\s*',
                r'ØªØ­Ù…ÙŠÙ„ Ù…Ø¨Ø§Ø´Ø±\s*', r'ØªÙ†Ø²ÙŠÙ„ Ù…Ø¨Ø§Ø´Ø±\s*', r'Ø­ØµØ±ÙŠØ§\s*', r'ÙÙ‚Ø·\s*',
                r'Ø§ÙˆÙ† Ù„Ø§ÙŠÙ†\s*', r'Ù…Ø¨Ø§Ø´Ø±Ø©\s*', r'Ø±ÙˆØ§Ø¨Ø· Ø³Ø±ÙŠØ¹Ø©\s*',
                r'Ø¨Ø¬ÙˆØ¯Ø©\s*(?:HD|FHD|4K|720p|1080p|BluRay|WEB-DL|HDRip|DVDRip|BDRip|WEBRip)?\s*',
                r'ÙƒØ§Ù…Ù„ ÙˆÙ…ØªØ±Ø¬Ù…\s*', r'Ù…ØªØ±Ø¬Ù… Ù„Ù„Ø¹Ø±Ø¨ÙŠØ©\s*', r'Ù…Ø¯Ø¨Ù„Ø¬\s*',
                r'Ø¨Ø¯ÙˆÙ† Ø¥Ø¹Ù„Ø§Ù†Ø§Øª\s*', r'Ø´Ø§Ù‡Ø¯ Ù…Ø¬Ø§Ù†Ø§\s*', r'Ù…Ø¬Ø§Ù†Ø§Ù‹\s*',
                r'Ø¬Ù…ÙŠØ¹ Ø­Ù„Ù‚Ø§Øª\s*', r'Ø§Ù„Ù…ÙˆØ³Ù… (?:Ø§Ù„Ø§ÙˆÙ„|Ø§Ù„Ø«Ø§Ù†ÙŠ|Ø§Ù„Ø«Ø§Ù„Ø«|Ø§Ù„Ø±Ø§Ø¨Ø¹|Ø§Ù„Ø®Ø§Ù…Ø³|Ø§Ù„Ø³Ø§Ø¯Ø³|Ø§Ù„Ø³Ø§Ø¨Ø¹|Ø§Ù„Ø«Ø§Ù…Ù†|Ø§Ù„ØªØ§Ø³Ø¹|Ø§Ù„Ø¹Ø§Ø´Ø±|Ø§Ù„Ø£ÙˆÙ„|Ø§Ù„Ø«Ø§Ù†ÙŠ|Ø§Ù„Ø«Ø§Ù„Ø«|Ø§Ù„Ø±Ø§Ø¨Ø¹|Ø§Ù„Ø®Ø§Ù…Ø³|Ø§Ù„Ø³Ø§Ø¯Ø³|Ø§Ù„Ø³Ø§Ø¨Ø¹|Ø§Ù„Ø«Ø§Ù…Ù†|Ø§Ù„ØªØ§Ø³Ø¹|Ø§Ù„Ø¹Ø§Ø´Ø±|\d+)\s*',
                r'Ø§ÙŠØ¬ÙŠ Ø¨Ø³Øª\s*', r'ÙˆÙŠ Ø³ÙŠÙ…Ø§\s*', r'Ù…Ø§ÙŠ Ø³ÙŠÙ…Ø§\s*', r'Ø³ÙŠÙ…Ø§ ÙƒÙ„ÙˆØ¨\s*',
                r'ØªÙƒØªÙˆÙƒ Ø³ÙŠÙ…Ø§\s*', r'Ø§ÙƒÙˆØ§Ù…\s*', r'Ø´Ø§Ù‡Ø¯ ÙÙˆØ± ÙŠÙˆ\s*', r'Ø§ÙÙ„Ø§Ù…ÙƒÙˆ\s*',
                r'Ø³ÙŠÙ…Ø§ ÙÙˆØ± ÙŠÙˆ\s*', r'ÙÙˆØ´Ø§Ø±\s*', r'Ø§ÙÙ„Ø§Ù…\s*', r'Ù…ÙˆÙ‚Ø¹ [Ø£-ÙŠ\w\s]*?\s*', 
                r'Ù‚ØµØ© (ÙÙŠÙ„Ù…|Ù…Ø³Ù„Ø³Ù„|Ø§Ù†Ù…ÙŠ)\s*(?:Ø¬Ø¯ÙŠØ¯)?\s*(?:ØªØ¯ÙˆØ± Ø§Ø­Ø¯Ø§Ø«)?\s*(?:Ø­ÙˆÙ„)?\s*', 
                r'ØªØ¯ÙˆØ± Ø§Ø­Ø¯Ø§Ø« (Ø§Ù„ÙÙŠÙ„Ù…|Ø§Ù„Ù…Ø³Ù„Ø³Ù„|Ø§Ù„Ø§Ù†Ù…ÙŠ)?\s*(?:Ø­ÙˆÙ„)?\s*',
                r'Ø§Ø­Ø¯Ø§Ø« (Ø§Ù„ÙÙŠÙ„Ù…|Ø§Ù„Ù…Ø³Ù„Ø³Ù„|Ø§Ù„Ø§Ù†Ù…ÙŠ)?\s*(?:Ø­ÙˆÙ„)?\s*',
                r'Ù…Ù„Ø®Øµ Ø§Ù„Ù‚ØµØ©\s*(?:Ø­ÙˆÙ„)?\s*',
                r'ØªØ¨Ø¯Ø£ Ø§Ù„Ø§Ø­Ø¯Ø§Ø« Ø¹Ù†Ø¯Ù…Ø§\s*',
                r'ÙÙŠÙ„Ù… (Ø¬Ø¯ÙŠØ¯|Ø­ØµØ±ÙŠ|Ø§Ù„Ø£Ù†)?\s*', r'Ù…Ø³Ù„Ø³Ù„ (Ø¬Ø¯ÙŠØ¯|Ø­ØµØ±ÙŠ|Ø§Ù„Ø£Ù†)?\s*', r'Ø§Ù†Ù…ÙŠ (Ø¬Ø¯ÙŠØ¯|Ø­ØµØ±ÙŠ|Ø§Ù„Ø£Ù†)?\s*',
                r'Ø£ÙÙ„Ø§Ù…202[0-9]|Ù…Ø³Ù„Ø³Ù„Ø§Øª202[0-9]|Ø£Ù†Ù…ÙŠ202[0-9]', 
                r'Ø§ÙˆÙ†Ù„Ø§ÙŠÙ†'
            ]
            
            for phrase in promo_phrases:
                description = re.sub(phrase, '', description, flags=re.IGNORECASE | re.DOTALL).strip()
            
            description = re.sub(r'^[^\w\s\u0600-\u06FF]+', '', description).strip()
            description = re.sub(r'[^\w\s\u0600-\u06FF]+$', '', description).strip()

            if movie_title_for_ref and description.lower().startswith(movie_title_for_ref.lower()):
                description = description[len(movie_title_for_ref):].strip()
                description = re.sub(r'^[^\w\s\u0600-\u06FF]+', '', description).strip() 

            description = re.sub(r'\s{2,}', ' ', description).strip()
            
            if len(description) > 500: 
                description = description[:497] + "..." 

        # Extract release year from the detail page
        year_tag = movie_soup.find("span", class_="year") # Example for Shahid4u
        if not year_tag:
            year_tag = movie_soup.find("div", class_="MovieInfo__Details__item", string=re.compile(r"Ø³Ù†Ø© Ø§Ù„Ø¥ØµØ¯Ø§Ø±|Year")) # Generic search
            if year_tag:
                year_match = re.search(r'(\d{4})', year_tag.get_text(strip=True))
                if year_match:
                    try:
                        release_year = int(year_match.group(1))
                    except ValueError:
                        release_year = None
        
        if year_tag and not release_year: # If year_tag exists but regex didn't catch it
            year_match = re.search(r'(\d{4})', year_tag.get_text(strip=True))
            if year_match:
                try:
                    release_year = int(year_match.group(1))
                except ValueError:
                    release_year = None


    except requests.exceptions.RequestException as e:
        logger.warning(f"âš ï¸ Ø®Ø·Ø£ ÙÙŠ Ø¬Ù„Ø¨ ØªÙØ§ØµÙŠÙ„ Ø§Ù„ÙÙŠÙ„Ù… Ù…Ù† {movie_url} Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… requests: {e}")
    except Exception as e:
        logger.warning(f"âš ï¸ Ø®Ø·Ø£ ØºÙŠØ± Ù…ØªÙˆÙ‚Ø¹ ÙÙŠ Ø¬Ù„Ø¨ ØªÙØ§ØµÙŠÙ„ Ø§Ù„ÙÙŠÙ„Ù… Ù…Ù† {movie_url}: {e}")
    
    return description, release_year


# --- Ø¯ÙˆØ§Ù„ Ø§Ù„ØªØ­Ù„ÙŠÙ„ (Extractors) Ù„ÙƒÙ„ Ù…ÙˆÙ‚Ø¹ ---

def parse_wecima(soup):
    movies = []
    for item in soup.select("div.GridItem"):
        try:
            link_tag = item.select_one("a")
            if not link_tag or not link_tag.get("href"):
                logger.debug(f"Wecima: Skipping item due to missing link or href: {item.prettify()}")
                continue
            link = link_tag["href"]
            
            title_tag = item.select_one("strong.hasyear") or item.select_one("img")
            raw_title = ""
            if title_tag:
                if title_tag.name == 'strong':
                    raw_title = title_tag.get_text(strip=True)
                else: # It's an img tag
                    raw_title = title_tag.get("alt", "N/A")
            if not raw_title or raw_title == "N/A":
                logger.debug(f"Wecima: Title not found or N/A for link {link} - Item HTML: {item.prettify()}")
                raw_title = "Ø¹Ù†ÙˆØ§Ù† ØºÙŠØ± Ù…ØªÙˆÙØ±" # Provide a default title

            image_url = None
            bg_style_tag = item.select_one("span.BG--GridItem")
            if bg_style_tag and 'data-lazy-style' in bg_style_tag.attrs:
                match = re.search(r'url\((.*?)\)', bg_style_tag['data-lazy-style'])
                if match:
                    image_url = match.group(1).strip("'\"")
            
            if not image_url:
                img_tag = item.select_one("img")
                if img_tag:
                    image_url = img_tag.get("data-src") or img_tag.get("src")
            if not image_url:
                logger.debug(f"Wecima: Image URL not found for title '{raw_title}' (link: {link}) - Item HTML: {item.prettify()}")
                image_url = "https://placehold.co/200x300/cccccc/333333?text=No+Image" # Placeholder

            movies.append({"title": clean_title(raw_title), "url": link, "image_url": image_url, "source": "Wecima"})
        except Exception as e:
            logger.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ ØªØ­Ù„ÙŠÙ„ Ø¹Ù†ØµØ± Wecima: {e} - Item HTML causing error: {item.prettify()}")
            continue
    return movies

def parse_topcinema(soup):
    movies = []
    for item in soup.select("div.col-lg-2.col-md-3.col-sm-4.col-xs-6.col-6.MovieBlock"):
        try:
            link_tag = item.select_one("a")
            if not link_tag or not link_tag.get("href"):
                logger.debug(f"TopCinema: Skipping item due to missing link or href: {item.prettify()}")
                continue
            link = link_tag["href"]
            
            title_tag = item.select_one("h2.Title")
            raw_title = title_tag.get_text(strip=True) if title_tag else "N/A"
            if not raw_title or raw_title == "N/A":
                logger.debug(f"TopCinema: Title not found or N/A for link {link} - Item HTML: {item.prettify()}")
                raw_title = "Ø¹Ù†ÙˆØ§Ù† ØºÙŠØ± Ù…ØªÙˆÙØ±"
            
            img_tag = item.select_one("img")
            image_url = img_tag.get("data-src") or img_tag.get("src") if img_tag else None
            if not image_url:
                logger.debug(f"TopCinema: Image URL not found for title '{raw_title}' (link: {link}) - Item HTML: {item.prettify()}")
                image_url = "https://placehold.co/200x300/cccccc/333333?text=No+Image"
            
            movies.append({"title": clean_title(raw_title), "url": link, "image_url": image_url, "source": "TopCinema"})
        except Exception as e:
            logger.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ ØªØ­Ù„ÙŠÙ„ Ø¹Ù†ØµØ± TopCinema: {e} - Item HTML causing error: {item.prettify()}")
            continue
    return movies

def parse_cimaclub(soup):
    movies = []
    for item in soup.select("div.Small--Box"):
        try:
            link_tag = item.select_one("a.recent--block")
            if not link_tag or not link_tag.get("href"):
                logger.debug(f"CimaClub: Skipping item due to missing link or href: {item.prettify()}")
                continue
            link = link_tag["href"]
            
            raw_title = "Ø¹Ù†ÙˆØ§Ù† ØºÙŠØ± Ù…ØªÙˆÙØ±" # Default value
            
            # Attempt 1: Get title from h2 within inner--title
            title_h2_tag = item.select_one(".inner--title h2")
            if title_h2_tag:
                extracted_title = title_h2_tag.get_text(strip=True)
                if extracted_title:
                    raw_title = extracted_title
            
            # Attempt 2: If h2 failed, try img alt attribute
            if raw_title == "Ø¹Ù†ÙˆØ§Ù† ØºÙŠØ± Ù…ØªÙˆÙØ±":
                img_tag_for_title = item.select_one("div.Poster img")
                if img_tag_for_title:
                    extracted_title = img_tag_for_title.get("alt", "")
                    if extracted_title:
                        raw_title = extracted_title
            
            # Attempt 3: If img alt failed, try link title attribute
            if raw_title == "Ø¹Ù†ÙˆØ§Ù† ØºÙŠØ± Ù…ØªÙˆÙØ±":
                extracted_title = link_tag.get("title", "")
                if extracted_title:
                    raw_title = extracted_title

            if raw_title == "Ø¹Ù†ÙˆØ§Ù† ØºÙŠØ± Ù…ØªÙˆÙØ±":
                logger.debug(f"CimaClub: Could not extract title for link {link} - Item HTML: {item.prettify()}")
                raw_title = "Ø¹Ù†ÙˆØ§Ù† ØºÙŠØ± Ù…ØªÙˆÙØ±" # Ensure default if all attempts fail

            img_tag = item.select_one("div.Poster img")
            image_url = img_tag.get("data-src") or img_tag.get("src") if img_tag else None
            if not image_url:
                logger.debug(f"CimaClub: Image URL not found for title '{raw_title}' (link: {link}) - Item HTML: {item.prettify()}")
                image_url = "https://placehold.co/200x300/cccccc/333333?text=No+Image" # Placeholder
            
            movies.append({"title": clean_title(raw_title), "url": link, "image_url": image_url, "source": "CimaClub"})
        except Exception as e:
            logger.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ ØªØ­Ù„ÙŠÙ„ Ø¹Ù†ØµØ± CimaClub: {e} - Item HTML causing error: {item.prettify()}")
            continue
    return movies

def parse_tuktukcima(soup):
    movies = []
    for item in soup.select("div.Blocks ul li.MovieBlock"):
        try:
            link_tag = item.select_one("a")
            if not link_tag or not link_tag.get("href"):
                logger.debug(f"TukTukCima: Skipping item due to missing link or href: {item.prettify()}")
                continue
            link = link_tag["href"]
            
            title_tag = item.select_one("h2.Title")
            raw_title = title_tag.get_text(strip=True) if title_tag else "N/A"
            if not raw_title or raw_title == "N/A":
                logger.debug(f"TukTukCima: Title not found or N/A for link {link} - Item HTML: {item.prettify()}")
                raw_title = "Ø¹Ù†ÙˆØ§Ù† ØºÙŠØ± Ù…ØªÙˆÙØ±"
            
            img_tag = item.select_one("img")
            image_url = img_tag.get("data-src") or img_tag.get("src") if img_tag else None
            if not image_url:
                logger.debug(f"TukTukCima: Image URL not found for title '{raw_title}' (link: {link}) - Item HTML: {item.prettify()}")
                image_url = "https://placehold.co/200x300/cccccc/333333?text=No+Image"
            
            movies.append({"title": clean_title(raw_title), "url": link, "image_url": image_url, "source": "TukTukCima"})
        except Exception as e:
            logger.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ ØªØ­Ù„ÙŠÙ„ Ø¹Ù†ØµØ± TukTukCima: {e} - Item HTML causing error: {item.prettify()}")
            continue
    return movies

def parse_egy_onl(soup):
    movies = []
    for item in soup.select("div.Blocks ul.MovieList div.movie-box"):
        try:
            link_tag = item.select_one("a")
            if not link_tag or not link_tag.get("href"):
                logger.debug(f"EgyBest: Skipping item due to missing link or href: {item.prettify()}")
                continue
            link = link_tag["href"]
            
            # Ø§Ù„Ø¹Ù†ÙˆØ§Ù† Ù…ÙˆØ¬ÙˆØ¯ ÙÙŠ alt Ù„Ù„ØµÙˆØ±Ø©
            title_tag = item.select_one("img")
            raw_title = title_tag.get("alt", "N/A") if title_tag else "N/A"
            if not raw_title or raw_title == "N/A":
                logger.debug(f"EgyBest: Title not found or N/A for link {link} - Item HTML: {item.prettify()}")
                raw_title = "Ø¹Ù†ÙˆØ§Ù† ØºÙŠØ± Ù…ØªÙˆÙØ±"
            
            image_url = title_tag.get("data-src") or title_tag.get("src") if title_tag else None
            if not image_url:
                logger.debug(f"EgyBest: Image URL not found for title '{raw_title}' (link: {link}) - Item HTML: {item.prettify()}")
                image_url = "https://placehold.co/200x300/cccccc/333333?text=No+Image"
            
            movies.append({"title": clean_title(raw_title), "url": link, "image_url": image_url, "source": "EgyBest"})
        except Exception as e:
            logger.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ ØªØ­Ù„ÙŠÙ„ Ø¹Ù†ØµØ± EgyBest: {e} - Item HTML causing error: {item.prettify()}")
            continue
    return movies


def parse_mycima(soup):
    movies = []
    for item in soup.select("div.GridItem"):
        try:
            link_tag = item.select_one("a")
            if not link_tag or not link_tag.get("href"):
                logger.debug(f"MyCima: Skipping item due to missing link or href: {item.prettify()}")
                continue
            link = link_tag["href"]
            
            title_tag = item.select_one("strong.hasyear") or item.select_one("img")
            raw_title = ""
            if title_tag:
                if title_tag.name == 'strong':
                    raw_title = title_tag.get_text(strip=True)
                else: # It's an img tag
                    raw_title = title_tag.get("alt", "N/A")
            if not raw_title or raw_title == "N/A":
                logger.debug(f"MyCima: Title not found or N/A for link {link} - Item HTML: {item.prettify()}")
                raw_title = "Ø¹Ù†ÙˆØ§Ù† ØºÙŠØ± Ù…ØªÙˆÙØ±"

            image_url = None
            bg_style_tag = item.select_one("span.BG--GridItem")
            if bg_style_tag and 'data-lazy-style' in bg_style_tag.attrs:
                match = re.search(r'url\((.*?)\)', bg_style_tag['data-lazy-style'])
                if match:
                    image_url = match.group(1).strip("'\"")
            
            if not image_url:
                img_tag = item.select_one("img")
                if img_tag:
                    image_url = img_tag.get("data-src") or img_tag.get("src")
            if not image_url:
                logger.debug(f"MyCima: Image URL not found for title '{raw_title}' (link: {link}) - Item HTML: {item.prettify()}")
                image_url = "https://placehold.co/200x300/cccccc/333333?text=No+Image"

            movies.append({"title": clean_title(raw_title), "url": link, "image_url": image_url, "source": "MyCima"})
        except Exception as e:
            logger.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ ØªØ­Ù„ÙŠÙ„ Ø¹Ù†ØµØ± MyCima: {e} - Item HTML causing error: {item.prettify()}")
            continue
    return movies

def parse_akoam(soup):
    movies = []
    for item in soup.select("div.movie-box"):
        try:
            link_tag = item.select_one("a")
            if not link_tag or not link_tag.get("href"):
                logger.debug(f"Akoam: Skipping item due to missing link or href: {item.prettify()}")
                continue
            link = link_tag["href"]
            
            title_tag = item.select_one("h2.Title") or item.select_one("img") # Title can be in h2 or img alt
            raw_title = ""
            if title_tag:
                if title_tag.name == 'h2':
                    raw_title = title_tag.get_text(strip=True)
                else: # It's an img tag
                    raw_title = title_tag.get("alt", "N/A")
            if not raw_title or raw_title == "N/A":
                logger.debug(f"Akoam: Title not found or N/A for link {link} - Item HTML: {item.prettify()}")
                raw_title = "Ø¹Ù†ÙˆØ§Ù† ØºÙŠØ± Ù…ØªÙˆÙØ±"
            
            img_tag = item.select_one("img")
            image_url = img_tag.get("data-src") or img_tag.get("src") if img_tag else None
            if not image_url:
                logger.debug(f"Akoam: Image URL not found for title '{raw_title}' (link: {link}) - Item HTML: {item.prettify()}")
                image_url = "https://placehold.co/200x300/cccccc/333333?text=No+Image"
            
            movies.append({"title": clean_title(raw_title), "url": link, "image_url": image_url, "source": "Akoam"})
        except Exception as e:
            logger.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ ØªØ­Ù„ÙŠÙ„ Ø¹Ù†ØµØ± Akoam: {e} - Item HTML causing error: {item.prettify()}")
            continue
    return movies

def parse_shahid4u(soup):
    movies = []
    for item in soup.select("div.GridItem"):
        try:
            link_tag = item.select_one("a.MovieBlock") # Specific anchor tag
            if not link_tag or not link_tag.get("href"):
                logger.debug(f"Shahid4u: Skipping item due to missing link or href: {item.prettify()}")
                continue
            link = link_tag["href"]
            
            title_tag = item.select_one("h2.MovieTitle")
            raw_title = title_tag.get_text(strip=True) if title_tag else "N/A"
            if not raw_title or raw_title == "N/A":
                logger.debug(f"Shahid4u: Title not found or N/A for link {link} - Item HTML: {item.prettify()}")
                raw_title = "Ø¹Ù†ÙˆØ§Ù† ØºÙŠØ± Ù…ØªÙˆÙØ±"
            
            img_tag = item.select_one("img")
            image_url = img_tag.get("src") if img_tag else None # Shahid4u ÙŠØ³ØªØ®Ø¯Ù… src Ù…Ø¨Ø§Ø´Ø±Ø©
            if not image_url:
                logger.debug(f"Shahid4u: Image URL not found for title '{raw_title}' (link: {link}) - Item HTML: {item.prettify()}")
                image_url = "https://placehold.co/200x300/cccccc/333333?text=No+Image"
            
            movies.append({"title": clean_title(raw_title), "url": link, "image_url": image_url, "source": "Shahid4u"})
        except Exception as e:
            logger.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ ØªØ­Ù„ÙŠÙ„ Ø¹Ù†ØµØ± Shahid4u: {e} - Item HTML causing error: {item.prettify()}")
            continue
    return movies

def parse_aflamco(soup):
    movies = []
    for item in soup.select("div.ModuleItem"):
        try:
            link_tag = item.select_one("a")
            if not link_tag or not link_tag.get("href"):
                logger.debug(f"Aflamco: Skipping item due to missing link or href: {item.prettify()}")
                continue
            link = link_tag["href"]
            
            title_tag = item.select_one("h2.ModuleTitle")
            raw_title = title_tag.get_text(strip=True) if title_tag else "N/A"
            if not raw_title or raw_title == "N/A":
                logger.debug(f"Aflamco: Title not found or N/A for link {link} - Item HTML: {item.prettify()}")
                raw_title = "Ø¹Ù†ÙˆØ§Ù† ØºÙŠØ± Ù…ØªÙˆÙØ±"
            
            img_tag = item.select_one("img")
            image_url = img_tag.get("data-src") or img_tag.get("src") if img_tag else None
            if not image_url:
                logger.debug(f"Aflamco: Image URL not found for title '{raw_title}' (link: {link}) - Item HTML: {item.prettify()}")
                image_url = "https://placehold.co/200x300/cccccc/333333?text=No+Image"
            
            movies.append({"title": clean_title(raw_title), "url": link, "image_url": image_url, "source": "Aflamco"})
        except Exception as e:
            logger.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ ØªØ­Ù„ÙŠÙ„ Ø¹Ù†ØµØ± Aflamco: {e} - Item HTML causing error: {item.prettify()}")
            continue
    return movies

def parse_cima4u(soup):
    movies = []
    for item in soup.select("div.MovieBlock"):
        try:
            link_tag = item.select_one("a")
            if not link_tag or not link_tag.get("href"):
                logger.debug(f"Cima4u: Skipping item due to missing link or href: {item.prettify()}")
                continue
            link = link_tag["href"]
            
            title_tag = item.select_one("h2.Title")
            raw_title = title_tag.get_text(strip=True) if title_tag else "N/A"
            if not raw_title or raw_title == "N/A":
                logger.debug(f"Cima4u: Title not found or N/A for link {link} - Item HTML: {item.prettify()}")
                raw_title = "Ø¹Ù†ÙˆØ§Ù† ØºÙŠØ± Ù…ØªÙˆÙØ±"
            
            img_tag = item.select_one("img")
            image_url = img_tag.get("data-src") or img_tag.get("src") if img_tag else None
            if not image_url:
                logger.debug(f"Cima4u: Image URL not found for title '{raw_title}' (link: {link}) - Item HTML: {item.prettify()}")
                image_url = "https://placehold.co/200x300/cccccc/333333?text=No+Image"
            
            movies.append({"title": clean_title(raw_title), "url": link, "image_url": image_url, "source": "Cima4u"})
        except Exception as e:
            logger.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ ØªØ­Ù„ÙŠÙ„ Ø¹Ù†ØµØ± Cima4u: {e} - Item HTML causing error: {item.prettify()}")
            continue
    return movies

def parse_fushaar(soup):
    movies = []
    for item in soup.select("div.Blocks .MovieBlock"):
        try:
            link_tag = item.select_one("a")
            if not link_tag or not link_tag.get("href"):
                logger.debug(f"Fushaar: Skipping item due to missing link or href: {item.prettify()}")
                continue
            link = link_tag["href"]
            
            title_tag = item.select_one("h2.Title")
            raw_title = title_tag.get_text(strip=True) if title_tag else "N/A"
            if not raw_title or raw_title == "N/A":
                logger.debug(f"Fushaar: Title not found or N/A for link {link} - Item HTML: {item.prettify()}")
                raw_title = "Ø¹Ù†ÙˆØ§Ù† ØºÙŠØ± Ù…ØªÙˆÙØ±"
            
            img_tag = item.select_one("img")
            image_url = img_tag.get("data-lazy-src") or img_tag.get("src") if img_tag else None
            if not image_url:
                logger.debug(f"Fushaar: Image URL not found for title '{raw_title}' (link: {link}) - Item HTML: {item.prettify()}")
                image_url = "https://placehold.co/200x300/cccccc/333333?text=No+Image"
            
            movies.append({"title": clean_title(raw_title), "url": link, "image_url": image_url, "source": "Fushaar"})
        except Exception as e:
            logger.error(f"âŒ Error parsing Fushaar item: {e} - Item HTML causing error: {item.prettify()}")
            continue
    return movies

def parse_aflaam(soup):
    movies = []
    for item in soup.select("div.movies-list-grid div.item"):
        try:
            link_tag = item.select_one("a.box")
            if not link_tag or not link_tag.get("href"):
                logger.debug(f"Aflaam: Skipping item due to missing link or href: {item.prettify()}")
                continue
            link = link_tag["href"]
            
            title_tag = item.select_one("h3.entry-title")
            raw_title = title_tag.get_text(strip=True) if title_tag else "N/A"
            if not raw_title or raw_title == "N/A":
                logger.debug(f"Aflaam: Title not found or N/A for link {link} - Item HTML: {item.prettify()}")
                raw_title = "Ø¹Ù†ÙˆØ§Ù† ØºÙŠØ± Ù…ØªÙˆÙØ±"
            
            img_tag = item.select_one("picture img.lazy") 
            image_url = img_tag.get("data-src") or img_tag.get("src") if img_tag else None
            if not image_url:
                logger.debug(f"Aflaam: Image URL not found for title '{raw_title}' (link: {link}) - Item HTML: {item.prettify()}")
                image_url = "https://placehold.co/200x300/cccccc/333333?text=No+Image"
            
            movies.append({"title": clean_title(raw_title), "url": link, "image_url": image_url, "source": "Aflaam"})
        except Exception as e:
            logger.error(f"âŒ Error parsing Aflaam item: {e} - Item HTML causing error: {item.prettify()}")
            continue
    return movies

def parse_egydead(soup):
    movies = []
    for item in soup.select("div.movie-box, div.GridItem, div.Blocks ul.MovieList div.movie-box"):
        try:
            link_tag = item.select_one("a")
            if not link_tag or not link_tag.get("href"):
                logger.debug(f"EgyDead: Skipping item due to missing link or href: {item.prettify()}")
                continue
            link = link_tag["href"]
            
            raw_title = "Ø¹Ù†ÙˆØ§Ù† ØºÙŠØ± Ù…ØªÙˆÙØ±"
            title_tag = item.select_one("h2.Title") or item.select_one("strong.hasyear") or item.select_one("img")
            if title_tag:
                if title_tag.name == 'img':
                    raw_title = title_tag.get("alt", "N/A")
                else:
                    raw_title = title_tag.get_text(strip=True)
            
            if not raw_title or raw_title == "N/A":
                logger.debug(f"EgyDead: Title not found or N/A for link {link} - Item HTML: {item.prettify()}")
                raw_title = "Ø¹Ù†ÙˆØ§Ù† ØºÙŠØ± Ù…ØªÙˆÙØ±"

            image_url = None
            img_tag = item.select_one("img")
            if img_tag:
                image_url = img_tag.get("data-src") or img_tag.get("src")
            if not image_url:
                logger.debug(f"EgyDead: Image URL not found for title '{raw_title}' (link: {link}) - Item HTML: {item.prettify()}")
                image_url = "https://placehold.co/200x300/cccccc/333333?text=No+Image"
            
            movies.append({"title": clean_title(raw_title), "url": link, "image_url": image_url, "source": "EgyDead"})
        except Exception as e:
            logger.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ ØªØ­Ù„ÙŠÙ„ Ø¹Ù†ØµØ± EgyDead: {e} - Item HTML causing error: {item.prettify()}")
            continue
    return movies


# --- Ù‚Ø§Ø¦Ù…Ø© Ø¨Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…ÙˆØ§Ù‚Ø¹ Ø§Ù„Ù…Ø¯Ø¹ÙˆÙ…Ø© (12 Ù…ÙˆÙ‚Ø¹Ù‹Ø§ ÙÙŠ Ù‡Ø°Ù‡ Ø§Ù„Ù†Ø³Ø®Ø©) ---
# Function to get the base URL (not used for specific URLs now)
def get_base_url(full_url):
    parsed_url = urlparse(full_url)
    return urlunparse((parsed_url.scheme, parsed_url.netloc, '', '', '', '')) + "/"

SCRAPERS = [
    {"name": "Wecima", "url": "https://wecima.video", "parser": parse_wecima, "category_hint": "mixed"},
    {"name": "TopCinema", "url": "https://web6.topcinema.cam/recent/", "parser": parse_topcinema, "category_hint": "mixed"},
    {"name": "CimaClub", "url": "https://cimaclub.day", "parser": parse_cimaclub, "category_hint": "mixed"},
    {"name": "TukTukCima", "url": "https://tuktukcima.art/recent/", "parser": parse_tuktukcima, "category_hint": "mixed"},
    {"name": "EgyBest", "url": "https://egy.onl/recent/", "parser": parse_egy_onl, "category_hint": "mixed"}, 
    {"name": "MyCima", "url": "https://mycima.video", "parser": parse_mycima, "category_hint": "mixed"},
    
    # Akoam
    {"name": "Akoam_Movies", "url": "https://akw.onl/movies/", "parser": parse_akoam, "category_hint": "ÙÙŠÙ„Ù…"},
    {"name": "Akoam_Series", "url": "https://akw.onl/series/", "parser": parse_akoam, "category_hint": "Ù…Ø³Ù„Ø³Ù„"},
    {"name": "Akoam_TV", "url": "https://akw.onl/tv/", "parser": parse_akoam, "category_hint": "Ù…Ø³Ù„Ø³Ù„"}, # Assuming TV is mostly series

    # Shahid4u
    {"name": "Shahid4u_Movies", "url": "https://shahed4uapp.com/page/movies/", "parser": parse_shahid4u, "category_hint": "ÙÙŠÙ„Ù…"},
    {"name": "Shahid4u_Series", "url": "https://shahed4uapp.com/page/series/", "parser": parse_shahid4u, "category_hint": "Ù…Ø³Ù„Ø³Ù„"},

    # Aflamco
    {"name": "Aflamco_Movies", "url": "https://aflamco.cloud/%D8%A7%D9%81%D9%84%D8%A7%D9%85/", "parser": parse_aflamco, "category_hint": "ÙÙŠÙ„Ù…"}, # Updated URL, assuming it's for movies

    # Cima4u (new domain and specific categories)
    {"name": "Cima4u_Movies", "url": "https://cema4u.vip/category/%d8%a7%d9%81%d9%84%d8%a7%d9%85-%d8%a7%d8%ac%d9%86%d8%a8%d9%8a/", "parser": parse_cima4u, "category_hint": "ÙÙŠÙ„Ù…"},
    {"name": "Cima4u_Series", "url": "https://cema4u.vip/category/%d9%85%d8%b3%d9%84%d8%b3%d9%84%d8%a7%d8%aa-%d8%a7%d8%ac%d9%86%d8%a8%d9%8a/", "parser": parse_cima4u, "category_hint": "Ù…Ø³Ù„Ø³Ù„"},

    {"name": "Fushaar", "url": "https://www.fushaar.com/?tlvaz", "parser": parse_fushaar, "category_hint": "mixed"}, # Updated URL

    # Aflaam
    {"name": "Aflaam_Movies", "url": "https://aflaam.com/movies", "parser": parse_aflaam, "category_hint": "ÙÙŠÙ„Ù…"},
    {"name": "Aflaam_Series", "url": "https://aflaam.com/series", "parser": parse_aflaam, "category_hint": "Ù…Ø³Ù„Ø³Ù„"},

    # New Site: EgyDead
    {"name": "EgyDead_Movies", "url": "https://egydead.video/category/%d8%a7%d9%81%d9%84%d8%a7%d9%85-%d8%a7%d8%ac%d9%86%d8%a8%d9%8a/", "parser": parse_egydead, "category_hint": "ÙÙŠÙ„Ù…"},
    {"name": "EgyDead_Series", "url": "https://egydead.video/series-category/%d9%85%d8%b3%d9%84%d8%b3%d9%84%d8%a7%d8%aa-%d8%a7%d8%ac%d9%86%d8%a8%d9%8a-1/", "parser": parse_egydead, "category_hint": "Ù…Ø³Ù„Ø³Ù„"},
]

# --- Ø¬Ù„Ø¨ ÙˆØªØ­Ù„ÙŠÙ„ Ù…Ø­ØªÙˆÙ‰ Ø§Ù„ØµÙØ­Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© Ù„Ù„Ù…ÙˆÙ‚Ø¹ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… requests ---
def scrape_single_main_page_and_parse(scraper: dict):
    site_name = scraper["name"]
    site_url = scraper["url"]
    parser_func = scraper["parser"]
    
    logger.info(f"Ø¬Ø§Ø±Ù ÙØ­Øµ Ø§Ù„ØµÙØ­Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© Ù„Ù€: {site_name} - {site_url}")
    
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/125.0.0.0 Safari/537.36'
        }
        response = requests.get(site_url, headers=headers, timeout=60) # Increased timeout
        response.raise_for_status() # Raise an exception for HTTP errors
        
        try:
            soup = BeautifulSoup(response.content, 'lxml') 
        except Exception as bs_e:
            logger.warning(f"LXML parser not available for {site_name}, falling back to html.parser: {bs_e}")
            soup = BeautifulSoup(response.content, 'html.parser')
        
        movies = parser_func(soup)

        if movies:
            logger.info(f"âœ… {len(movies)} ÙÙŠÙ„Ù… ØªÙ… Ø§Ø³ØªØ®Ø±Ø§Ø¬Ù‡ Ù…Ø¨Ø¯Ø¦ÙŠØ§Ù‹ Ù…Ù† {site_name}")
        else:
            logger.warning(f"âš ï¸ Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø£ÙÙ„Ø§Ù… ÙÙŠ {site_name} (Ø§Ù„ØµÙØ­Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©) Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù…Ø­Ø¯Ø¯Ø§Øª Ø§Ù„Ø­Ø§Ù„ÙŠØ©.")
        return movies
    except requests.exceptions.RequestException as e:
        logger.error(f"âŒ Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ Ø¬Ù„Ø¨/ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØµÙØ­Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© Ù„Ù€ {site_name} ({site_url}) Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… requests: {e}")
        return []
    except Exception as e:
        logger.error(f"âŒ Ø®Ø·Ø£ ØºÙŠØ± Ù…ØªÙˆÙ‚Ø¹ Ø£Ø«Ù†Ø§Ø¡ Ø¬Ù„Ø¨/ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØµÙØ­Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© Ù„Ù€ {site_name} ({site_url}): {e}")
        return []

# --- Ø¬Ù…Ø¹ Ø§Ù„Ø£ÙÙ„Ø§Ù… Ù…Ù† Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…ÙˆØ§Ù‚Ø¹ ÙˆØªØ­Ø¯ÙŠØ« Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª (UPSERT) ---
def scrape_movies_and_get_new(): # Ù„ÙŠØ³Øª Ø¯Ø§Ù„Ø© async Ø¨Ø¹Ø¯ Ø§Ù„Ø¢Ù†
    newly_added_movies = [] 
    total_processed_count = 0 
    
    conn = sqlite3.connect('movies.db') 
    c = conn.cursor()

    # Step 1: Scrape main pages to get initial movie links and basic info
    all_initial_movies_flat = []
    for scraper_info in SCRAPERS:
        movies_from_site = scrape_single_main_page_and_parse(scraper_info)
        # Add source and category_hint to each movie for later processing
        for movie in movies_from_site:
            movie['source_name_for_logging'] = scraper_info['name'] # Store original scraper name for logging
            movie['category_hint'] = scraper_info.get('category_hint')
        all_initial_movies_flat.extend(movies_from_site)
        time.sleep(1) # Polite delay between sites

    # Step 2: Process each initial movie, visit its detail page, and add/update in DB
    for movie_initial_data in all_initial_movies_flat:
        total_processed_count += 1
        try:
            cleaned_title_text = clean_title(movie_initial_data["title"])
            current_time_str = datetime.now().strftime('%Y-%m-%d %H:%M:%S')

            # Check if movie exists in DB
            c.execute("SELECT id, title, image_url, category, description, release_year FROM movies WHERE url = ?", (movie_initial_data["url"],))
            existing_movie_db = c.fetchone()

            detailed_description = ""
            accurate_release_year = None

            # Only visit detail page if movie is potentially new or needs updating its description/year
            # or if the description/release_year from DB is missing
            if not existing_movie_db or not existing_movie_db[4] or not existing_movie_db[5]: 
                detailed_description, accurate_release_year = extract_detailed_movie_info_requests(
                    movie_initial_data["url"], cleaned_title_text
                )
                time.sleep(0.5) # Polite delay for detail pages

            # Use extracted data or fallback to initial data/existing DB data
            movie_description = detailed_description if detailed_description else (existing_movie_db[4] if existing_movie_db else "")
            movie_release_year = accurate_release_year if accurate_release_year else (existing_movie_db[5] if existing_movie_db else None)
            # If release_year is still None, try to extract from initial title (fallback)
            if not movie_release_year:
                year_match = re.search(r'(\d{4})', movie_initial_data["title"])
                if year_match:
                    try:
                        movie_release_year = int(year_match.group(1))
                    except ValueError:
                        movie_release_year = None

            # Use the category hint from the scraper definition
            category = deduce_category(cleaned_title_text, movie_initial_data["url"], movie_initial_data.get("category_hint"))

            # Update site status for the source of this movie
            # Call update_site_status function
            update_site_status(movie_initial_data["source_name_for_logging"], 'active')

            if existing_movie_db:
                db_id, old_title, old_image_url, old_category, old_description, old_release_year = existing_movie_db

                changed = False
                if old_title != cleaned_title_text: changed = True
                if old_image_url != movie_initial_data.get("image_url"): changed = True
                if old_category != category: changed = True
                if old_description != movie_description: changed = True
                if old_release_year != movie_release_year: changed = True

                if changed:
                    c.execute("""
                        UPDATE movies 
                        SET title = ?, image_url = ?, category = ?, description = ?, release_year = ?, last_updated = ?
                        WHERE url = ?
                    """, (cleaned_title_text, movie_initial_data.get("image_url"), category,
                          movie_description, movie_release_year, current_time_str, movie_initial_data["url"]))
                    logger.info(f"âœ… ØªÙ… ØªØ­Ø¯ÙŠØ« Ø§Ù„ÙÙŠÙ„Ù…: {cleaned_title_text} Ù…Ù† {movie_initial_data['source_name_for_logging']}")
            else:
                # Insert new movie
                c.execute("INSERT INTO movies (title, url, source, image_url, category, description, release_year, last_updated) VALUES (?, ?, ?, ?, ?, ?, ?, ?)",
                          (cleaned_title_text, movie_initial_data["url"], movie_initial_data["source_name_for_logging"], movie_initial_data.get("image_url"),
                           category, movie_description, movie_release_year, current_time_str))
                newly_added_movies.append({
                    "title": cleaned_title_text,
                    "url": movie_initial_data["url"],
                    "source": movie_initial_data["source_name_for_logging"],
                    "image_url": movie_initial_data.get("image_url"),
                    "category": category,
                    "description": movie_description,
                    "release_year": movie_release_year
                })
                logger.info(f"âœ¨ ØªÙ… Ø¥Ø¶Ø§ÙØ© ÙÙŠÙ„Ù… Ø¬Ø¯ÙŠØ¯: {cleaned_title_text} Ù…Ù† {movie_initial_data['source_name_for_logging']}")

        except Exception as e:
            logger.error(f"  âŒ Ø®Ø·Ø£ ÙÙŠ Ù…Ø¹Ø§Ù„Ø¬Ø© ÙÙŠÙ„Ù… Ù…Ù† {movie_initial_data.get('source_name_for_logging', 'N/A')} ({movie_initial_data.get('title', 'N/A')}): {e}")
        finally:
            conn.commit() 
            # No asyncio.sleep here as this is a synchronous function now

    conn.close()
    logger.info(f"âœ… ØªÙ… Ù…Ø¹Ø§Ù„Ø¬Ø© {total_processed_count} ÙÙŠÙ„Ù… ÙÙŠ Ù‡Ø°Ù‡ Ø§Ù„Ø¬ÙˆÙ„Ø©. {len(newly_added_movies)} Ù…Ù†Ù‡Ø§ Ø¬Ø¯ÙŠØ¯Ø©.")
    return newly_added_movies

# --- Ø¯Ø§Ù„Ø© Ù…Ø³Ø§Ø¹Ø¯Ø© Ù„ØªØ­Ø¯ÙŠØ« Ø­Ø§Ù„Ø© Ø§Ù„Ù…ÙˆÙ‚Ø¹ ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ---
def update_site_status(site_name, status):
    conn = sqlite3.connect('movies.db')
    c = conn.cursor()
    current_time_str = datetime.now().strftime('%Y-%m-%d %H:%M:%S.%f')
    c.execute("INSERT OR REPLACE INTO site_status (site_name, last_scraped, status) VALUES (?, ?, ?)",
              (site_name, current_time_str, status))
    conn.commit()
    conn.close()

# --- Ø¯Ø§Ù„Ø© Ù…Ø³Ø§Ø¹Ø¯Ø© Ù„Ø¬Ù„Ø¨ Ø­Ø§Ù„Ø© Ø§Ù„Ù…ÙˆØ§Ù‚Ø¹ Ù…Ù† Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ---
def get_site_statuses():
    conn = sqlite3.connect('movies.db')
    c = conn.cursor()
    c.execute("SELECT site_name, last_scraped, status FROM site_status")
    statuses = c.fetchall()
    conn.close()
    return statuses

# --- Ø¥Ø±Ø³Ø§Ù„ Ø§Ù„Ø£ÙÙ„Ø§Ù… Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø© Ù„Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ† ---
async def send_new_movies(context: ContextTypes.DEFAULT_TYPE): 
    # Ø§Ø³ØªØ®Ø¯Ø§Ù… asyncio.to_thread Ù„ØªØ´ØºÙŠÙ„ Ø¯Ø§Ù„Ø© Ø§Ù„Ø¬Ù„Ø¨ Ø§Ù„Ù…ØªØ²Ø§Ù…Ù†Ø© ÙÙŠ Ù…Ø¤Ø´Ø± ØªØ±Ø§Ø¨Ø· Ù…Ù†ÙØµÙ„
    new_movies_to_send = await asyncio.to_thread(scrape_movies_and_get_new)
    if not new_movies_to_send:
        logger.info("Ù„Ø§ ØªÙˆØ¬Ø¯ Ø£ÙÙ„Ø§Ù… Ø¬Ø¯ÙŠØ¯Ø© Ù„Ù„Ø¥Ø±Ø³Ø§Ù„ ÙÙŠ Ù‡Ø°Ù‡ Ø§Ù„Ø¬ÙˆÙ„Ø©.")
        return

    conn = sqlite3.connect('movies.db') 
    c = conn.cursor()
    c.execute("SELECT user_id, receive_movies, receive_series, receive_anime FROM users")
    users_with_prefs = c.fetchall()
    conn.close()

    for user_id, receive_movies, receive_series, receive_anime in users_with_prefs:
        try:
            filtered_movies = []
            for movie in new_movies_to_send: 
                if (movie['category'] == 'ÙÙŠÙ„Ù…' and receive_movies) or \
                   (movie['category'] == 'Ù…Ø³Ù„Ø³Ù„' and receive_series) or \
                   (movie['category'] == 'Ø£Ù†Ù…ÙŠ' and receive_anime):
                    filtered_movies.append(movie)
            
            if not filtered_movies:
                continue 

            await context.bot.send_message(
                chat_id=user_id,
                text="ğŸ¬ <b>Ø£ÙÙ„Ø§Ù… Ø¬Ø¯ÙŠØ¯Ø© Ù…ØªØ§Ø­Ø©:</b>\n\n",
                parse_mode='HTML'
            )
            await asyncio.sleep(0.5) 

            for movie in filtered_movies: 
                escaped_title = html.escape(movie['title'])

                photo_caption_text = (
                    f"ğŸ¬ <b>Ø§Ù„Ø¹Ù†ÙˆØ§Ù†:</b> {escaped_title}\n"
                )
                if movie['release_year']:
                    photo_caption_text += f"ğŸ“… <b>Ø³Ù†Ø© Ø§Ù„Ø¥ØµØ¯Ø§Ø±:</b> {movie['release_year']}\n"
                photo_caption_text += (
                    f"ğŸ¬ <b>Ø§Ù„Ù…ØµØ¯Ø±:</b> {movie['source']}\n"
                    f"ğŸ¬ <b>Ø§Ù„ÙØ¦Ø©:</b> {movie['category']}\n"
                )
                
                if movie['description']:
                    description_text = movie['description'].strip()
                    if description_text:
                        photo_caption_text += f"\nğŸ“ <b>Ø§Ù„ÙˆØµÙ:</b> {description_text}\n"
                
                keyboard = [[InlineKeyboardButton("Ø§Ø¶ØºØ· Ù‡Ù†Ø§ Ù„Ù„Ù…Ø´Ø§Ù‡Ø¯Ø©", url=movie["url"])]]
                reply_markup = InlineKeyboardMarkup(keyboard)

                image_to_send = movie['image_url'] if movie['image_url'] else "https://placehold.co/600x400/cccccc/333333?text=No+Image+Available"

                try:
                    await context.bot.send_photo(
                        chat_id=user_id,
                        photo=image_to_send,
                        caption=photo_caption_text, 
                        parse_mode='HTML',
                        reply_markup=reply_markup 
                    )
                    await asyncio.sleep(0.3) 

                except Exception as photo_e:
                    logger.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø¥Ø±Ø³Ø§Ù„ Ø§Ù„ØµÙˆØ±Ø© Ù„Ù„Ù…Ø³ØªØ®Ø¯Ù… {user_id} Ù„Ù„ÙÙŠÙ„Ù… {movie['title']}: {photo_e}")
                    fallback_text = (
                        f"ğŸ¬ <b>Ø§Ù„Ø¹Ù†ÙˆØ§Ù†:</b> {escaped_title}\n"
                    )
                    if movie['release_year']:
                        fallback_text += f"ğŸ“… <b>Ø³Ù†Ø© Ø§Ù„Ø¥ØµØ¯Ø§Ø±:</b> {movie['release_year']}\n"
                    fallback_text += (
                        f"ğŸ¬ <b>Ø§Ù„Ù…ØµØ¯Ø±:</b> {movie['source']}\n"
                        f"ğŸ¬ <b>Ø§Ù„ÙØ¦Ø©:</b> {movie['category']}\n"
                    )
                    if movie['description']:
                        fallback_text += f"\nğŸ“ <b>Ø§Ù„ÙˆØµÙ:</b> {movie['description'].strip()}\n"
                    fallback_text += f'\nğŸ”— <b>Ø±Ø§Ø¨Ø· Ø§Ù„Ù…Ø´Ø§Ù‡Ø¯Ø©:</b> <a href="{movie["url"]}">Ø§Ø¶ØºØ· Ù‡Ù†Ø§ Ù„Ù„Ù…Ø´Ø§Ù‡Ø¯Ø©</a>'
                    
                    await context.bot.send_message(
                        chat_id=user_id,
                        text=fallback_text,
                        parse_mode='HTML',
                        disable_web_page_preview=True
                    )
                    await asyncio.sleep(0.3)

        except Exception as e:
            logger.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø¥Ø±Ø³Ø§Ù„ Ø§Ù„Ø£ÙÙ„Ø§Ù… Ù„Ù„Ù…Ø³ØªØ®Ø¯Ù… {user_id}: {e}")

# --- Self-Ping function ---
async def self_ping_async():
    try:
        # Pinging the local Flask server to keep the Repl alive
        response = requests.get("http://localhost:8080", timeout=10)
        response.raise_for_status()
        logger.info(f"âœ… Self-ping successful! Status: {response.status_code}")
    except requests.exceptions.RequestException as e:
        logger.error(f"âŒ Self-ping failed: {e}")

# --- Ø¯Ø§Ù„Ø© Ù…Ø³Ø§Ø¹Ø¯Ø© Ù„Ø¥Ø±Ø³Ø§Ù„ Ø±Ø³Ø§Ù„Ø© Ø§Ù„Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© Ù…Ø¹ Ø§Ù„Ø£Ø²Ø±Ø§Ø± Ø§Ù„Ø¯Ø§Ø¦Ù…Ø© ---
async def main_menu_internal(chat_id: int, context: ContextTypes.DEFAULT_TYPE, user_first_name: str = "Ø¹Ø²ÙŠØ²ÙŠ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…"):
    keyboard = [
        [KeyboardButton("âš™ï¸ Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„ØªÙ†Ø¨ÙŠÙ‡Ø§Øª")],
        [KeyboardButton("ğŸ”„ ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø¢Ù†")]
    ]
    # Ø§Ø³ØªØ®Ø¯Ø§Ù… ReplyKeyboardMarkup Ù„Ø¬Ø¹Ù„ Ø§Ù„Ø£Ø²Ø±Ø§Ø± Ø¯Ø§Ø¦Ù…Ø©
    reply_markup = ReplyKeyboardMarkup(keyboard, resize_keyboard=True, one_time_keyboard=False)

    welcome_msg = (
        f"ğŸ‰ Ù…Ø±Ø­Ø¨Ø§Ù‹ {user_first_name}!\n"
        "Ø£Ù†Ø§ Ø¨ÙˆØª Ø§Ù„Ø£ÙÙ„Ø§Ù… Ø§Ù„Ø°ÙƒÙŠØŒ Ø³Ø£Ø±Ø³Ù„ Ù„Ùƒ Ø£Ø­Ø¯Ø« Ø§Ù„Ø£ÙÙ„Ø§Ù… ØªÙ„Ù‚Ø§Ø¦ÙŠØ§Ù‹ Ù…Ù† 12 Ù…ÙˆÙ‚Ø¹ Ø³ÙŠÙ†Ù…Ø§Ø¦ÙŠ:\n"
        "- Wecima, TopCinema, CimaClub, TukTukCima, EgyBest, MyCima,\n"
        "- Akoam, Shahid4u, Aflamco, Cima4u, Fushaar, Aflaam.\n\n"
        "â° Ø³ÙŠØµÙ„Ùƒ ØªØ­Ø¯ÙŠØ« Ø¨Ø§Ù„Ø£ÙÙ„Ø§Ù… Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø© ÙƒÙ„ 6 Ø³Ø§Ø¹Ø§Øª ØªÙ„Ù‚Ø§Ø¦ÙŠØ§Ù‹.\n" 
        "Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø£Ø²Ø±Ø§Ø± Ø£Ø¯Ù†Ø§Ù‡ Ù„Ù„ØªØ­ÙƒÙ… ÙÙŠ Ø§Ù„Ø¨ÙˆØª."
    )
    # Ø¥Ø±Ø³Ø§Ù„ Ø±Ø³Ø§Ù„Ø© Ø¬Ø¯ÙŠØ¯Ø© Ù…Ø¹ Ù„ÙˆØ­Ø© Ø§Ù„Ù…ÙØ§ØªÙŠØ­ Ø§Ù„Ø¯Ø§Ø¦Ù…Ø©
    await context.bot.send_message(chat_id=chat_id, text=welcome_msg, parse_mode='HTML', reply_markup=reply_markup)


# --- Ù…Ø¹Ø§Ù„Ø¬ Ø£Ù…Ø± /settings (ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ¯Ø¹Ø§Ø¤Ù‡ Ù„ØªØ¹Ø¯ÙŠÙ„ Ø§Ù„Ø±Ø³Ø§Ù„Ø© Ø£Ùˆ Ø¥Ø±Ø³Ø§Ù„Ù‡Ø§ Ù„Ø£ÙˆÙ„ Ù…Ø±Ø©) ---
async def settings_command_internal(chat_id: int, context: ContextTypes.DEFAULT_TYPE, message_id: int = None, edit_mode: bool = False):
    user_prefs = get_user_preferences(chat_id)
    
    movies_status = "âœ… Ù…ÙØ¹Ù„" if user_prefs["movies"] else "âŒ Ù…Ø¹Ø·Ù„"
    series_status = "âœ… Ù…ÙØ¹Ù„" if user_prefs["series"] else "âŒ Ù…Ø¹Ø·Ù„"
    anime_status = "âœ… Ù…ÙØ¹Ù„" if user_prefs["anime"] else "âŒ Ù…Ø¹Ø·Ù„"

    settings_text = (
        "âš™ï¸ <b>Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„ØªÙ†Ø¨ÙŠÙ‡Ø§Øª:</b>\n"
        "Ø§Ø®ØªØ± Ø£Ù†ÙˆØ§Ø¹ Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„ØªÙŠ ØªØ±ØºØ¨ ÙÙŠ ØªÙ„Ù‚ÙŠ ØªÙ†Ø¨ÙŠÙ‡Ø§Øª Ø¹Ù†Ù‡Ø§:\n\n"
        f"â€¢ Ø§Ù„Ø£ÙÙ„Ø§Ù…: {movies_status}\n"
        f"â€¢ Ø§Ù„Ù…Ø³Ù„Ø³Ù„Ø§Øª: {series_status}\n"
        f"â€¢ Ø§Ù„Ø£Ù†Ù…ÙŠ: {anime_status}\n\n"
        "Ø§Ø¶ØºØ· Ø¹Ù„Ù‰ Ø§Ù„Ø²Ø± Ù„ØªØºÙŠÙŠØ± Ø§Ù„Ø­Ø§Ù„Ø©."
    )

    keyboard = [
        [InlineKeyboardButton(f"Ø§Ù„Ø£ÙÙ„Ø§Ù…: {movies_status}", callback_data='toggle_movies')],
        [InlineKeyboardButton(f"Ø§Ù„Ù…Ø³Ù„Ø³Ù„Ø§Øª: {series_status}", callback_data='toggle_series')],
        [InlineKeyboardButton(f"Ø§Ù„Ø£Ù†Ù…ÙŠ: {anime_status}", callback_data='toggle_anime')],
        [InlineKeyboardButton("â¬…ï¸ Ø±Ø¬ÙˆØ¹", callback_data='back_to_main_menu')] 
    ]
    reply_markup = InlineKeyboardMarkup(keyboard)

    if edit_mode and message_id:
        try:
            await context.bot.edit_message_text(
                chat_id=chat_id,
                message_id=message_id,
                text=settings_text,
                parse_mode='HTML',
                reply_markup=reply_markup
            )
        except Exception as e:
            logger.error(f"Ø®Ø·Ø£ ÙÙŠ ØªØ¹Ø¯ÙŠÙ„ Ø±Ø³Ø§Ù„Ø© Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ù„Ù„Ù…Ø³ØªØ®Ø¯Ù… {chat_id} (Ø±Ø³Ø§Ù„Ø© {message_id}): {e}")
            await context.bot.send_message(chat_id=chat_id, text=settings_text, parse_mode='HTML', reply_markup=reply_markup)
    else:
        await context.bot.send_message(chat_id=chat_id, text=settings_text, parse_mode='HTML', reply_markup=reply_markup)

# --- Ù…Ø¹Ø§Ù„Ø¬Ø§Øª Ø£ÙˆØ§Ù…Ø± Ø§Ù„Ø¨ÙˆØª ---
async def start(update: Update, context: ContextTypes.DEFAULT_TYPE):
    user = update.effective_user
    add_user(user.id, user.username, user.first_name, user.last_name) 
    # Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ main_menu_internal Ù„Ø¥Ø¸Ù‡Ø§Ø± Ø§Ù„Ø£Ø²Ø±Ø§Ø± Ø§Ù„Ø¯Ø§Ø¦Ù…Ø©
    await main_menu_internal(user.id, context, user_first_name=user.first_name)

async def button_callback_handler(update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:
    query = update.callback_query
    await query.answer() 

    chat_id = query.message.chat_id 

    if query.data == 'settings_command':
        # Ù‡Ø°Ø§ Ù„Ù† ÙŠØªÙ… Ø§Ø³ØªØ¯Ø¹Ø§Ø¤Ù‡ Ù…Ø¨Ø§Ø´Ø±Ø© Ù…Ù† Ø§Ù„Ø£Ø²Ø±Ø§Ø± Ø§Ù„Ø¯Ø§Ø¦Ù…Ø©ØŒ Ø¨Ù„ Ù…Ù† Ø²Ø± "Ø±Ø¬ÙˆØ¹" ÙÙŠ Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª
        await settings_command_internal(chat_id, context, query.message.message_id, edit_mode=True) 
    elif query.data.startswith('toggle_'):
        pref_type = query.data.replace('toggle_', '')
        current_prefs = get_user_preferences(chat_id)
        new_value = 0 if current_prefs.get(pref_type) else 1
        
        if update_user_preference(chat_id, f"receive_{pref_type}", new_value):
            await settings_command_internal(chat_id, context, query.message.message_id, edit_mode=True)
        else:
            await context.bot.send_message(chat_id=chat_id, text="Ø­Ø¯Ø« Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ ØªØ­Ø¯ÙŠØ« Ø§Ù„ØªÙØ¶ÙŠÙ„Ø§Øª.")
    elif query.data == 'back_to_main_menu':
        user = update.effective_user
        # Ø¹Ù†Ø¯ Ø§Ù„Ø¹ÙˆØ¯Ø© Ù…Ù† Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§ØªØŒ Ù†Ø¹Ø±Ø¶ Ø§Ù„Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© Ø¨Ø§Ù„Ø£Ø²Ø±Ø§Ø± Ø§Ù„Ø¯Ø§Ø¦Ù…Ø©
        await main_menu_internal(chat_id, context, user_first_name=user.first_name)
        # Ù†Ø­Ø°Ù Ø±Ø³Ø§Ù„Ø© Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ù‚Ø¯ÙŠÙ…Ø© Ù„ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ø¯Ø±Ø¯Ø´Ø©
        try:
            await context.bot.delete_message(chat_id=chat_id, message_id=query.message.message_id)
        except Exception as e:
            logger.warning(f"Failed to delete settings message: {e}")

# --- Ù…Ø¹Ø§Ù„Ø¬ Ø§Ù„Ø²Ø± Ø§Ù„Ø¯Ø§Ø¦Ù… "Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„ØªÙ†Ø¨ÙŠÙ‡Ø§Øª" ---
async def handle_settings_button(update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:
    chat_id = update.effective_chat.id
    # Ø¥Ø±Ø³Ø§Ù„ Ø±Ø³Ø§Ù„Ø© Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø© (Ù…Ø¹ Ø§Ù„Ø£Ø²Ø±Ø§Ø± Ø§Ù„Ø¯Ø§Ø®Ù„ÙŠØ©)
    await settings_command_internal(chat_id, context)

# --- Ù…Ø¹Ø§Ù„Ø¬ Ø§Ù„Ø²Ø± Ø§Ù„Ø¯Ø§Ø¦Ù… "ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø¢Ù†" ---
async def handle_manual_update_button(update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:
    chat_id = update.effective_chat.id
    await update.message.reply_text("Ø¨Ø¯Ø¡ ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø£ÙÙ„Ø§Ù… ÙŠØ¯ÙˆÙŠØ§Ù‹... Ù‚Ø¯ ÙŠØ³ØªØºØ±Ù‚ Ø§Ù„Ø£Ù…Ø± Ø¨Ø¶Ø¹ Ø¯Ù‚Ø§Ø¦Ù‚.", parse_mode='HTML')
    try:
        await send_new_movies(context)
        await update.message.reply_text("âœ… ØªÙ… Ø§Ù„Ø§Ù†ØªÙ‡Ø§Ø¡ Ù…Ù† Ø§Ù„ØªØ­Ø¯ÙŠØ« Ø§Ù„ÙŠØ¯ÙˆÙŠ Ù„Ù„Ø£ÙÙ„Ø§Ù….", parse_mode='HTML')
    except Exception as e:
        logger.exception("Ø®Ø·Ø£ ÙØ§Ø¯Ø­ Ø£Ø«Ù†Ø§Ø¡ Ø§Ù„ØªØ­Ø¯ÙŠØ« Ø§Ù„ÙŠØ¯ÙˆÙŠ Ù„Ù„Ø£ÙÙ„Ø§Ù….")
        error_message_for_user = f"âŒ Ø­Ø¯Ø« Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ Ø§Ù„ØªØ­Ø¯ÙŠØ« Ø§Ù„ÙŠØ¯ÙˆÙŠ Ù„Ù„Ø£ÙÙ„Ø§Ù…. Ø§Ù„ØªÙØ§ØµÙŠÙ„: <code>{html.escape(str(e))[:150]}...</code>"
        await update.message.reply_text(error_message_for_user, parse_mode='HTML')
        if ADMIN_CHAT_ID:
            try:
                await context.bot.send_message(
                    chat_id=ADMIN_CHAT_ID,
                    text=f"âš ï¸ Ø®Ø·Ø£ ÙÙŠ Ø§Ù„ØªØ­Ø¯ÙŠØ« Ø§Ù„ÙŠØ¯ÙˆÙŠ Ù„Ø¨ÙˆØª Ø§Ù„Ø£ÙÙ„Ø§Ù…: \n<code>{html.escape(str(e))}</code>",
                    parse_mode='HTML'
                )
            except Exception as admin_e:
                    logger.error(f"ÙØ´Ù„ Ø¥Ø±Ø³Ø§Ù„ Ø±Ø³Ø§Ù„Ø© Ø§Ù„Ø®Ø·Ø£ Ø¥Ù„Ù‰ Ø§Ù„Ù…Ø´Ø±Ù: {admin_e}")


async def show_site_status(update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:
    if str(update.effective_user.id) != ADMIN_CHAT_ID:
        await update.message.reply_text("Ù„ÙŠØ³ Ù„Ø¯ÙŠÙƒ Ø§Ù„ØµÙ„Ø§Ø­ÙŠØ© Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù‡Ø°Ø§ Ø§Ù„Ø£Ù…Ø±.")
        return

    statuses = get_site_statuses()
    message = "ğŸ“Š <b>Ø­Ø§Ù„Ø© Ø§Ù„Ù…ÙˆØ§Ù‚Ø¹:</b>\n\n"
    if not statuses:
        message += "Ù„Ø§ ØªÙˆØ¬Ø¯ Ø¨ÙŠØ§Ù†Ø§Øª Ø­Ø§Ù„Ø© Ù„Ù„Ù…ÙˆØ§Ù‚Ø¹ Ø¨Ø¹Ø¯."
    else:
        for site_name, last_scraped, status in statuses:
            if isinstance(last_scraped, str):
                try:
                    last_scraped_dt = datetime.strptime(last_scraped, '%Y-%m-%d %H:%M:%S.%f')
                except ValueError:
                    last_scraped_dt = datetime.strptime(last_scraped.split('.')[0], '%Y-%m-%d %H:%M:%S') 
            else:
                last_scraped_dt = last_scraped

            last_scraped_str = last_scraped_dt.strftime('%Y-%m-%d %H:%M')
            message += f"<b>{site_name}</b>: Ø¢Ø®Ø± Ø¬Ù„Ø¨: {last_scraped_str}, Ø§Ù„Ø­Ø§Ù„Ø©: {status}\n"
    
    await update.message.reply_text(message, parse_mode='HTML')

# --- Ø¯Ø§Ù„Ø© Ù„ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ø£ÙÙ„Ø§Ù… Ø§Ù„Ù‚Ø¯ÙŠÙ…Ø© Ù…Ù† Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ---
def cleanup_old_movies():
    conn = sqlite3.connect('movies.db')
    c = conn.cursor()
    
    # Ø­Ø°Ù Ø§Ù„Ø£ÙÙ„Ø§Ù… Ø§Ù„Ø£Ù‚Ø¯Ù… Ù…Ù† 90 ÙŠÙˆÙ…Ø§Ù‹
    ninety_days_ago = datetime.now() - timedelta(days=90)
    c.execute("DELETE FROM movies WHERE last_updated < ?", (ninety_days_ago,))
    deleted_count = c.rowcount
    
    # ØªÙ†ÙÙŠØ° VACUUM Ù„Ø¥Ø¹Ø§Ø¯Ø© Ø§Ø³ØªØµÙ„Ø§Ø­ Ø§Ù„Ù…Ø³Ø§Ø­Ø© Ø¨Ø¹Ø¯ Ø§Ù„Ø­Ø°Ù
    try:
        c.execute("VACUUM")
        logger.info("âœ… ØªÙ… ØªÙ†ÙÙŠØ° VACUUM Ø¹Ù„Ù‰ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¨Ù†Ø¬Ø§Ø­.")
    except Exception as e:
        logger.error(f"âŒ Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ ØªÙ†ÙÙŠØ° VACUUM Ø¹Ù„Ù‰ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª: {e}")

    conn.commit()
    conn.close()
    logger.info(f"âœ… ØªÙ… Ø­Ø°Ù {deleted_count} ÙÙŠÙ„Ù…Ù‹Ø§ Ù‚Ø¯ÙŠÙ…Ù‹Ø§ Ù…Ù† Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª.")


# --- Ù…Ù‡Ù…Ø© Ø§Ù„Ø¬Ø¯ÙˆÙ„Ø© ---
def schedule_job(application):
    loop = asyncio.new_event_loop()
    asyncio.set_event_loop(loop)

    async def run_async_task_wrapper_send_new_movies():
        try:
            await send_new_movies(application) 
        except Exception as e:
            logger.error(f"Ø®Ø·Ø£ ÙÙŠ Ù…Ù‡Ù…Ø© Ø¥Ø±Ø³Ø§Ù„ Ø§Ù„Ø£ÙÙ„Ø§Ù… Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø© Ø§Ù„Ù…Ø¬Ø¯ÙˆÙ„Ø©: {e}")

    async def run_async_task_wrapper_self_ping():
        try:
            await self_ping_async()
        except Exception as e:
            logger.error(f"Ø®Ø·Ø£ ÙÙŠ Ù…Ù‡Ù…Ø© Self-Ping Ø§Ù„Ù…Ø¬Ø¯ÙˆÙ„Ø©: {e}")

    # Ø¬Ø¯ÙˆÙ„Ø© Ù…Ù‡Ù…Ø© Ø¬Ù…Ø¹ ÙˆØ¥Ø±Ø³Ø§Ù„ Ø§Ù„Ø£ÙÙ„Ø§Ù… Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø© ÙƒÙ„ 6 Ø³Ø§Ø¹Ø§Øª.
    schedule.every(6).hours.do(lambda: asyncio.run_coroutine_threadsafe(run_async_task_wrapper_send_new_movies(), loop))
    
    # Ø¬Ø¯ÙˆÙ„Ø© ØªÙ†Ø¸ÙŠÙ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ÙŠÙˆÙ…ÙŠØ§Ù‹ ÙÙŠ ÙˆÙ‚Øª Ù…Ø¹ÙŠÙ† (Ù…Ø«Ù„Ø§Ù‹ 3 ØµØ¨Ø§Ø­Ø§Ù‹)
    schedule.every().day.at("03:00").do(cleanup_old_movies) 

    # Ø¬Ø¯ÙˆÙ„Ø© Self-Ping ÙƒÙ„ 5 Ø¯Ù‚Ø§Ø¦Ù‚
    schedule.every(5).minutes.do(lambda: asyncio.run_coroutine_threadsafe(run_async_task_wrapper_self_ping(), loop))

    logger.info("Ø¨Ø¯Ø¡ Ø¹Ù…Ù„ÙŠØ© Ø¬Ù…Ø¹ Ø§Ù„Ø£ÙÙ„Ø§Ù… Ø§Ù„Ø£ÙˆÙ„ÙŠØ©...")
    # ØªØ´ØºÙŠÙ„ Ø£ÙˆÙ„ÙŠ Ù„Ø¬Ù…Ø¹ Ø§Ù„Ø£ÙÙ„Ø§Ù… Ø¹Ù†Ø¯ Ø¨Ø¯Ø¡ Ø§Ù„Ø¨ÙˆØª
    asyncio.run_coroutine_threadsafe(run_async_task_wrapper_send_new_movies(), loop)
    # ØªØ´ØºÙŠÙ„ Ø£ÙˆÙ„ÙŠ Ù„Ù€ Self-Ping Ø¹Ù†Ø¯ Ø¨Ø¯Ø¡ Ø§Ù„Ø¨ÙˆØª
    asyncio.run_coroutine_threadsafe(run_async_task_wrapper_self_ping(), loop)

    while True:
        schedule.run_pending()
        time.sleep(30) 

# --- ØªÙ†ÙÙŠØ° Ø§Ù„Ø¨ÙˆØª Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ ---
def main():
    init_db() 
    
    global application
    application = Application.builder().token(TOKEN).build()
    
    application.add_handler(CommandHandler("start", start))
    application.add_handler(CommandHandler("sitestatus", show_site_status)) # Admin command
    application.add_handler(CallbackQueryHandler(button_callback_handler)) # For inline buttons (settings menu)

    # New handlers for permanent keyboard buttons
    application.add_handler(MessageHandler(filters.Regex(r"^âš™ï¸ Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„ØªÙ†Ø¨ÙŠÙ‡Ø§Øª$"), handle_settings_button))
    application.add_handler(MessageHandler(filters.Regex(r"^ğŸ”„ ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø¢Ù†$"), handle_manual_update_button))


    threading.Thread(target=schedule_job, args=(application,), daemon=True).start()

    logger.info("âœ… Ø¨ÙˆØª Ø§Ù„Ø£ÙÙ„Ø§Ù… ÙŠØ¹Ù…Ù„ Ø§Ù„Ø¢Ù† Ù…Ø¹ 12 Ù…ÙˆÙ‚Ø¹ Ø³ÙŠÙ†Ù…Ø§Ø¦ÙŠ (Ù…Ø¯Ø¹ÙˆÙ… Ø¨Ù€ Requests Ùˆ BeautifulSoup).") 
    logger.info("â±ï¸ ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø£ÙÙ„Ø§Ù… ÙƒÙ„ 6 Ø³Ø§Ø¹Ø§Øª ØªÙ„Ù‚Ø§Ø¦ÙŠØ§Ù‹ ÙˆØ®ÙŠØ§Ø± Ø§Ù„ØªØ­Ø¯ÙŠØ« Ø§Ù„ÙŠØ¯ÙˆÙŠ Ù…ØªØ§Ø­.") 
    logger.info("ğŸŒ Ø®Ø§Ø¯Ù… Keep-Alive ÙŠØ¹Ù…Ù„ Ø¹Ù„Ù‰ Ø§Ù„Ù…Ù†ÙØ° 8080.")
    logger.info("âš™ï¸ Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø£ÙˆØ§Ù…Ø± Ù…Ø«Ù„ /start Ùˆ /settings Ù„Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ† Ùˆ /sitestatus Ù„Ù„Ù…Ø´Ø±Ù.") 
    application.run_polling()

if __name__ == '__main__':
    main()
